{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DWzS3wXGtWRT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Evqqe4pG8k3z",
    "outputId": "b9003d1d-4fba-40f4-aa63-58deb401a5c8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  8 01:33:18 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3090         Off| 00000000:01:00.0  On |                  N/A |\r\n",
      "| 75%   69C    P0              163W / 350W|    568MiB / 24576MiB |      7%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      2319      G   /usr/lib/xorg/Xorg                          366MiB |\r\n",
      "|    0   N/A  N/A      2542      G   /usr/bin/gnome-shell                         36MiB |\r\n",
      "|    0   N/A  N/A      3374      G   ...7147389,14359769800334640508,262144      130MiB |\r\n",
      "|    0   N/A  N/A     88066      G   /usr/bin/nvidia-settings                      0MiB |\r\n",
      "|    0   N/A  N/A    233846      G   ...sion,SpareRendererForSitePerProcess       30MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ni3W9xIGXG1Z"
   },
   "source": [
    "## Load GoEmotions and General Audio Datasets (CREMA, TESS,  RAVDASS, ETC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VJWwfnsutyBN"
   },
   "outputs": [],
   "source": [
    "train_audio = pkl.load(open('./data/c4ai_clip/train_audio.pkl', \"rb\"))[['path', 'label']]\n",
    "test_audio = pkl.load(open('./data/c4ai_clip/test_audio.pkl', \"rb\"))[['path', 'label']]\n",
    "train_text = pkl.load(open('./data/c4ai_clip/train_text.pkl', \"rb\"))[['text', 'grouped_label']]\n",
    "test_text = pkl.load(open('./data/c4ai_clip/test_text.pkl', \"rb\"))[['text', 'grouped_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ov6R_5UhfUd7",
    "outputId": "9d35f7b5-a38e-4b6e-8be2-daf38365486e"
   },
   "outputs": [],
   "source": [
    "#!unzip ./data/c4ai_clip/audio_emo_resampled.zip -d ./audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kwdbm_LN0wVk"
   },
   "outputs": [],
   "source": [
    "def norm_labels(x):\n",
    "    if x == \"afraid\":\n",
    "        return \"fear\"\n",
    "    elif x == \"angry\":\n",
    "        return \"anger\"\n",
    "    elif x == \"disgusted\":\n",
    "        return \"disgust\"\n",
    "    elif x == \"sad\":\n",
    "        return \"sadness\"\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XefjoUugGJe9"
   },
   "outputs": [],
   "source": [
    "train_audio[\"label\"] = train_audio[\"label\"].apply(norm_labels)\n",
    "test_audio[\"label\"] = test_audio[\"label\"].apply(norm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "pIcNuJ6sjPt8",
    "outputId": "71110be1-2732-4d72-e8fa-3023f6f9632b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11545</th>\n",
       "      <td>./audio/audio_emo/crema.man.sad.465.wav</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>./audio/audio_emo/ravdass.man.sad.74.wav</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6326</th>\n",
       "      <td>./audio/audio_emo/tess.woman.surprised.370.wav</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11579</th>\n",
       "      <td>./audio/audio_emo/tess.woman.neutral.110.wav</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9502</th>\n",
       "      <td>./audio/audio_emo/crema.woman.happy.586.wav</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8732</th>\n",
       "      <td>./audio/audio_emo/crema.man.angry.530.wav</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5386</th>\n",
       "      <td>./audio/audio_emo/tess.woman.happy.322.wav</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>./audio/audio_emo/crema.man.afraid.455.wav</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6755</th>\n",
       "      <td>./audio/audio_emo/crema.woman.disgusted.239.wav</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9152</th>\n",
       "      <td>./audio/audio_emo/crema.woman.afraid.168.wav</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2337 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  path     label\n",
       "11545          ./audio/audio_emo/crema.man.sad.465.wav   sadness\n",
       "1970          ./audio/audio_emo/ravdass.man.sad.74.wav   sadness\n",
       "6326    ./audio/audio_emo/tess.woman.surprised.370.wav  surprise\n",
       "11579     ./audio/audio_emo/tess.woman.neutral.110.wav   neutral\n",
       "9502       ./audio/audio_emo/crema.woman.happy.586.wav       joy\n",
       "...                                                ...       ...\n",
       "8732         ./audio/audio_emo/crema.man.angry.530.wav     anger\n",
       "5386        ./audio/audio_emo/tess.woman.happy.322.wav       joy\n",
       "83          ./audio/audio_emo/crema.man.afraid.455.wav      fear\n",
       "6755   ./audio/audio_emo/crema.woman.disgusted.239.wav   disgust\n",
       "9152      ./audio/audio_emo/crema.woman.afraid.168.wav      fear\n",
       "\n",
       "[2337 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qk8YSzYOX4H6"
   },
   "source": [
    "## Load Meld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eDL1tDN8MLvT"
   },
   "outputs": [],
   "source": [
    "def load_split_meld(split):\n",
    "    assert split in ['train', 'test', 'dev']\n",
    "    \n",
    "    df = pd.read_csv(f\"./meld_raw/{split}_splits/{split}_sent_emo.csv\")\n",
    "    df['path'] = df.apply(lambda x: f\"./meld_raw/{split}_splits/audios/dia{x['Dialogue_ID']}_utt{x['Utterance_ID']}.wav\".strip('\\n'), axis=1)\n",
    "    \n",
    "    return (\n",
    "        df[['path', 'Emotion', 'Utterance']]\n",
    "        .rename(columns={'Emotion':'label', 'Utterance':'text'})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDKLMwOQX-6Z",
    "outputId": "ba1fa968-71e1-4f41-a9c0-42367a4d7a89"
   },
   "outputs": [],
   "source": [
    "df_meld_train = load_split_meld('train')\n",
    "df_meld_dev = load_split_meld('dev')\n",
    "df_meld_test = load_split_meld('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>./meld_raw/dev_splits/audios/dia4_utt1.wav</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Yeah, we wereÂ…",
       "we were just looking around.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          path    label  \\\n",
       "28  ./meld_raw/dev_splits/audios/dia4_utt1.wav  neutral   \n",
       "\n",
       "                                          text  \n",
       "28  Yeah, we were\n",
       "we were just looking around.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meld_dev[df_meld_dev['path'] == './meld_raw/dev_splits/audios/dia4_utt1.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>./meld_raw/train_splits/audios/dia1_utt8.wav</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Yeah, sure!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            path    label         text\n",
       "20  ./meld_raw/train_splits/audios/dia1_utt8.wav  neutral  Yeah, sure!"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meld_train[df_meld_train['path'] == './meld_raw/train_splits/audios/dia1_utt8.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_processed_utt_meld(df):\n",
    "    dialogue_utt_text = {}\n",
    "    for i, row in df.iterrows():\n",
    "        dialogue_utt = row[\"path\"].split(\"/\")[4].replace(\".wav\", \"\")\n",
    "        diag, utt = dialogue_utt.split(\"_\")\n",
    "        if diag not in dialogue_utt_text:\n",
    "            dialogue_utt_text[diag] = []\n",
    "        dialogue_utt_text[diag].append((utt, row[\"text\"]))\n",
    "    utterances_with_context = []\n",
    "    for i, row in df.iterrows():\n",
    "        dialogue_utt = row[\"path\"].split(\"/\")[4].replace(\".wav\", \"\")\n",
    "        diag, utt = dialogue_utt.split(\"_\")\n",
    "\n",
    "        texts = [t[1] for t in dialogue_utt_text[diag]]\n",
    "        texts_n = [t[0] for t in dialogue_utt_text[diag]]\n",
    "        \n",
    "        utt_n = int(utt.replace(\"utt\", \"\"))\n",
    "        \n",
    "        try:\n",
    "            if len(texts) == 1:\n",
    "                text_utt = \"[SEP]\" + texts[utt_n] + \"[SEP]\"\n",
    "                utterances_with_context.append(text_utt)\n",
    "                continue\n",
    "\n",
    "            if utt_n == 0 or utt_n == len(texts) - 1:\n",
    "                if utt_n == 0:\n",
    "                    text_utt = \"[SEP] \" + texts[utt_n] + \" [SEP] \" + texts[utt_n + 1]\n",
    "                else:\n",
    "                    text_utt = texts[utt_n - 1] + \" [SEP] \" + texts[utt_n] + \" [SEP]\"\n",
    "            else:\n",
    "                text_utt = texts[utt_n - 1] + \" [SEP] \" + texts[utt_n] + \" [SEP] \" + texts[utt_n + 1]\n",
    "        except:\n",
    "            text_utt = \"[SEP] \" + texts[texts_n.index(utt)] + \" [SEP]\"\n",
    "            \n",
    "        utterances_with_context.append(text_utt)\n",
    "    return utterances_with_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_meld_train = get_pre_processed_utt_meld(df_meld_train)\n",
    "processed_meld_dev = get_pre_processed_utt_meld(df_meld_dev)\n",
    "processed_meld_test = get_pre_processed_utt_meld(df_meld_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meld_train[\"text\"] = processed_meld_train\n",
    "df_meld_dev[\"text\"] = processed_meld_dev\n",
    "df_meld_test[\"text\"] = processed_meld_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./meld_raw/dev_splits/audios/dia0_utt0.wav</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[SEP] Oh my God, heÂ’s lost it. HeÂ’s totally lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./meld_raw/dev_splits/audios/dia0_utt1.wav</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Oh my God, heÂ’s lost it. HeÂ’s totally lost it....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./meld_raw/dev_splits/audios/dia1_utt0.wav</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[SEP] Or! Or, we could go to the bank, close o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./meld_raw/dev_splits/audios/dia1_utt1.wav</td>\n",
       "      <td>joy</td>\n",
       "      <td>Or! Or, we could go to the bank, close our acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./meld_raw/dev_splits/audios/dia1_utt2.wav</td>\n",
       "      <td>sadness</td>\n",
       "      <td>YouÂ’re a genius! [SEP] Aww, man, now we wonÂ’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>./meld_raw/dev_splits/audios/dia113_utt9.wav</td>\n",
       "      <td>sadness</td>\n",
       "      <td>I mean weÂ’re not, weÂ’re not gonna live togethe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>./meld_raw/dev_splits/audios/dia113_utt10.wav</td>\n",
       "      <td>sadness</td>\n",
       "      <td>No. [SEP] What? Oh my God! IÂ’m gonna miss you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>./meld_raw/dev_splits/audios/dia113_utt11.wav</td>\n",
       "      <td>sadness</td>\n",
       "      <td>What? Oh my God! IÂ’m gonna miss you so much! [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>./meld_raw/dev_splits/audios/dia113_utt12.wav</td>\n",
       "      <td>sadness</td>\n",
       "      <td>IÂ’m gonna miss you! [SEP] I mean itÂ’s the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>./meld_raw/dev_splits/audios/dia113_utt13.wav</td>\n",
       "      <td>sadness</td>\n",
       "      <td>I mean itÂ’s the end of an era! [SEP] I know! [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1109 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               path     label  \\\n",
       "0        ./meld_raw/dev_splits/audios/dia0_utt0.wav   sadness   \n",
       "1        ./meld_raw/dev_splits/audios/dia0_utt1.wav  surprise   \n",
       "2        ./meld_raw/dev_splits/audios/dia1_utt0.wav   neutral   \n",
       "3        ./meld_raw/dev_splits/audios/dia1_utt1.wav       joy   \n",
       "4        ./meld_raw/dev_splits/audios/dia1_utt2.wav   sadness   \n",
       "...                                             ...       ...   \n",
       "1104   ./meld_raw/dev_splits/audios/dia113_utt9.wav   sadness   \n",
       "1105  ./meld_raw/dev_splits/audios/dia113_utt10.wav   sadness   \n",
       "1106  ./meld_raw/dev_splits/audios/dia113_utt11.wav   sadness   \n",
       "1107  ./meld_raw/dev_splits/audios/dia113_utt12.wav   sadness   \n",
       "1108  ./meld_raw/dev_splits/audios/dia113_utt13.wav   sadness   \n",
       "\n",
       "                                                   text  \n",
       "0     [SEP] Oh my God, heÂ’s lost it. HeÂ’s totally lo...  \n",
       "1     Oh my God, heÂ’s lost it. HeÂ’s totally lost it....  \n",
       "2     [SEP] Or! Or, we could go to the bank, close o...  \n",
       "3     Or! Or, we could go to the bank, close our acc...  \n",
       "4     YouÂ’re a genius! [SEP] Aww, man, now we wonÂ’t ...  \n",
       "...                                                 ...  \n",
       "1104  I mean weÂ’re not, weÂ’re not gonna live togethe...  \n",
       "1105  No. [SEP] What? Oh my God! IÂ’m gonna miss you ...  \n",
       "1106  What? Oh my God! IÂ’m gonna miss you so much! [...  \n",
       "1107  IÂ’m gonna miss you! [SEP] I mean itÂ’s the end ...  \n",
       "1108  I mean itÂ’s the end of an era! [SEP] I know! [...  \n",
       "\n",
       "[1109 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meld_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBZMZhUUYR9K"
   },
   "source": [
    "## Load Iemocap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZaM9ftMbYUFq"
   },
   "outputs": [],
   "source": [
    "def load_iemocap(path):\n",
    "    def normalize_labels(label):\n",
    "        if label == 'neu':\n",
    "            return 'neutral'\n",
    "        elif label == 'sad' or label == 'fru':\n",
    "            return 'sadness'\n",
    "        elif label == 'fea':\n",
    "            return 'fear'\n",
    "        elif label == 'dis':\n",
    "            return 'disgust'\n",
    "        elif label == 'sur':\n",
    "            return 'surprise'\n",
    "        elif label == 'ang':\n",
    "            return 'anger'\n",
    "        elif label == 'hap' or label == 'exc':\n",
    "            return 'joy'\n",
    "        else:\n",
    "            return 'xxx'\n",
    "        \n",
    "    df = pd.read_csv(f'{path}/df_iemocap.csv')[['wav_file', 'emotion']]\n",
    "    df_trans = pd.read_csv(f'{path}/iemocapTrans.csv')[['titre', 'to_translate']]\n",
    "    \n",
    "    df = df.merge(df_trans.rename(columns={'titre':'wav_file', 'to_translate':'text'}), how='left', on='wav_file')\n",
    "    \n",
    "    df['label'] = df['emotion'].apply(normalize_labels)\n",
    "    print(df['label'].unique())\n",
    "    df = df[df['label'] != 'xxx']\n",
    "    df = df[['text', 'wav_file', 'label']]\n",
    "    df['wav_file'] = df['wav_file'].apply(lambda x: f\"{path}/audios/{x}.wav\")\n",
    "    df['split'] = df['wav_file'].apply(lambda x: 'train' if not '05' in x.split('_')[0][:-1] else 'test')\n",
    "    df = df.rename(columns={'wav_file':'path'})\n",
    "    train, test = df[df['split'] == 'train'], df[df['split'] == 'test']\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4Q4N-E0j0G-",
    "outputId": "ca0420e1-efd4-4dbf-843c-19c253fd877e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'xxx' 'sadness' 'anger' 'joy' 'surprise' 'fear' 'disgust']\n"
     ]
    }
   ],
   "source": [
    "df_iemocap_train, df_iemocap_test = load_iemocap('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excuse me.</td>\n",
       "      <td>./audios/Ses01F_impro01_F000.wav</td>\n",
       "      <td>neutral</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah.</td>\n",
       "      <td>./audios/Ses01F_impro01_F001.wav</td>\n",
       "      <td>neutral</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is there a problem?</td>\n",
       "      <td>./audios/Ses01F_impro01_F002.wav</td>\n",
       "      <td>neutral</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Well what's the problem?  Let me change it.</td>\n",
       "      <td>./audios/Ses01F_impro01_F005.wav</td>\n",
       "      <td>neutral</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What?  I'm getting an ID.  This is why I'm he...</td>\n",
       "      <td>./audios/Ses01F_impro01_F006.wav</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>No, you're not.</td>\n",
       "      <td>./audios/Ses04M_script03_2_M052.wav</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7865</th>\n",
       "      <td>Shut up. Shut up.</td>\n",
       "      <td>./audios/Ses04M_script03_2_M053.wav</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>You're a mean, evil minded</td>\n",
       "      <td>./audios/Ses04M_script03_2_M054.wav</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7867</th>\n",
       "      <td>little vampire And I hope to God I never set ...</td>\n",
       "      <td>./audios/Ses04M_script03_2_M055.wav</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7868</th>\n",
       "      <td>Whore.</td>\n",
       "      <td>./audios/Ses04M_script03_2_M056.wav</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5879 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0                                            Excuse me.   \n",
       "1                                                 Yeah.   \n",
       "2                                   Is there a problem?   \n",
       "5           Well what's the problem?  Let me change it.   \n",
       "6      What?  I'm getting an ID.  This is why I'm he...   \n",
       "...                                                 ...   \n",
       "7864                                    No, you're not.   \n",
       "7865                                  Shut up. Shut up.   \n",
       "7866                         You're a mean, evil minded   \n",
       "7867   little vampire And I hope to God I never set ...   \n",
       "7868                                             Whore.   \n",
       "\n",
       "                                     path    label  split  \n",
       "0        ./audios/Ses01F_impro01_F000.wav  neutral  train  \n",
       "1        ./audios/Ses01F_impro01_F001.wav  neutral  train  \n",
       "2        ./audios/Ses01F_impro01_F002.wav  neutral  train  \n",
       "5        ./audios/Ses01F_impro01_F005.wav  neutral  train  \n",
       "6        ./audios/Ses01F_impro01_F006.wav  sadness  train  \n",
       "...                                   ...      ...    ...  \n",
       "7864  ./audios/Ses04M_script03_2_M052.wav    anger  train  \n",
       "7865  ./audios/Ses04M_script03_2_M053.wav    anger  train  \n",
       "7866  ./audios/Ses04M_script03_2_M054.wav    anger  train  \n",
       "7867  ./audios/Ses04M_script03_2_M055.wav    anger  train  \n",
       "7868  ./audios/Ses04M_script03_2_M056.wav    anger  train  \n",
       "\n",
       "[5879 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iemocap_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mo7VlMCHj_zs",
    "outputId": "5678a9a9-252b-4d29-ecdd-43a310c5165d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'sadness', 'anger', 'joy', 'surprise', 'fear',\n",
       "       'disgust'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iemocap_train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'path', 'label', 'split'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iemocap_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXbw47lffnBL"
   },
   "source": [
    "## Join datasets (audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3qhKJxnIv2ma"
   },
   "outputs": [],
   "source": [
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "RJQ3Ky9xkew8"
   },
   "outputs": [],
   "source": [
    "df_train_audio = pd.concat([df_iemocap_train[['text', 'path', 'label']], df_meld_train, train_audio.assign(text=[None for _ in range(len(train_audio))])], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_audio = pd.concat([df_iemocap_test[['text', 'path', 'label']], df_meld_dev, test_audio.assign(text=[None for _ in range(len(test_audio))])], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNjRZbK4xcOB",
    "outputId": "b8589e93-9a56-494f-a9f8-6e38a4672f01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25213"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYBBNw5txuTr",
    "outputId": "a9b71212-2cdb-43f7-c1d8-626bfef56dff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5096"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dev_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### There are missing files (I believe in meld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "M54RINMGv6I-"
   },
   "outputs": [],
   "source": [
    "aud = []\n",
    "for f in df_train_audio[\"path\"]:\n",
    "    if f is None:\n",
    "        continue\n",
    "    if not exists(f):\n",
    "        aud.append(f)\n",
    "df_train_audio = df_train_audio[~df_train_audio['path'].isin(aud)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "uvrD-suPxfGf"
   },
   "outputs": [],
   "source": [
    "aud = []\n",
    "for f in df_dev_audio[\"path\"]:\n",
    "    if f is None:\n",
    "        continue\n",
    "    if not exists(f):\n",
    "        aud.append(f)\n",
    "df_dev_audio = df_dev_audio[~df_dev_audio['path'].isin(aud)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25212"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjAzuLrExx8Y",
    "outputId": "12a588d1-7ed1-4c03-fc9a-55a40911a149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5095"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dev_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXyZj8XMmMFv",
    "outputId": "becfddb8-9202-49b9-898b-7651d39e0da6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43410 25212 5427 5095\n"
     ]
    }
   ],
   "source": [
    "print(len(train_text), len(df_train_audio), len(test_text), len(df_dev_audio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "2g91PVAy96pN",
    "outputId": "420c9794-6818-489e-97f0-2a83ba3e7309"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lab_encoder = LabelEncoder()\n",
    "lab_encoder.fit(df_train_audio['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "XLc2EtfJvKuB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HwhtSIqFvWKo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.2, contrast_mode='all',\n",
    "                 base_temperature=0.2):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None, temperature=None, base_temperature=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        if temperature == None:\n",
    "            temperature = self.temperature\n",
    "        if base_temperature == None:\n",
    "            base_temperature = self.base_temperature\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 1]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (temperature/base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "YJdcGkcLd7PN"
   },
   "outputs": [],
   "source": [
    "class EmbeddingPropagation(nn.Module):\n",
    "    \"\"\"Embedding Propagation\"\"\"\n",
    "    def __init__(self, in_dim=2048, k=32, num_classes=7, eps=1e-8, keep_rate=0.8, alpha=0.1):\n",
    "        super(EmbeddingPropagation, self).__init__()\n",
    "        self.k = k\n",
    "        self.keep_rate = keep_rate\n",
    "        self.num_classes = num_classes\n",
    "        #self.g_enc = nn.Sequential(nn.Linear(in_dim, graph_emb))\n",
    "        self.sigma_enc = nn.Sequential(nn.Linear(in_dim, in_dim//4), nn.GELU(), nn.Linear(in_dim//4, 1))\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.tensor([alpha]).cuda(0), requires_grad=True)\n",
    "        #self.alpha = nn.Parameter(torch.tensor([alpha]), requires_grad=True)\n",
    "\n",
    "    def graph_construction(self, inputs):\n",
    "        # Get Graph Embeddings\n",
    "        #emb_all = inputs\n",
    "        N, d    = inputs.shape[0], inputs.shape[1]\n",
    "\n",
    "        sigmas = self.sigma_enc(inputs)\n",
    "        \n",
    "        # Get adjacency matrix\n",
    "        emb_all_sigma = inputs / (sigmas+self.eps) # N*d\n",
    "        W = torch.cdist(emb_all_sigma, emb_all_sigma)\n",
    "        W = torch.exp(-W/2)\n",
    "\n",
    "        # Keep topk nodes for neighborhood\n",
    "        _, indices = torch.topk(W, self.k + 1)\n",
    "        \n",
    "        # Drop diagonal\n",
    "        indices = indices[:, 1:]\n",
    "        \n",
    "        # Construct Undirected Graph adjacency\n",
    "        mask = torch.zeros_like(W)\n",
    "        mask = mask.scatter(1, indices, 1)\n",
    "        mask = ((mask+torch.t(mask))>0).type(torch.float32)      # union, kNN graph\n",
    "        W = W * mask\n",
    "\n",
    "        # Dropout edges\n",
    "        if self.training == True and self.keep_rate < 1.0:\n",
    "            dropout_mask = torch.rand(*W.shape, requires_grad=True).cuda() < self.keep_rate # Can go wrong if mask entirely zero\n",
    "            #dropout_mask = torch.rand(*W.shape, requires_grad=True) < self.keep_rate\n",
    "            W = W * dropout_mask\n",
    "            \n",
    "        # Graph Adjacency matrix normalization\n",
    "        D = W.sum(0)\n",
    "        D_sqrt_inv = torch.sqrt(1.0/(D+self.eps))\n",
    "        D1 = torch.unsqueeze(D_sqrt_inv,1).repeat(1,N)\n",
    "        D2 = torch.unsqueeze(D_sqrt_inv,0).repeat(N,1)\n",
    "        S = D1*W*D2\n",
    "\n",
    "        return S\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        N = inputs.shape[0]\n",
    "\n",
    "        S = self.graph_construction(inputs)\n",
    "        inv = torch.inverse(torch.eye(N).cuda(0)-self.alpha*S + self.eps)\n",
    "        x = torch.matmul(inv, inputs)\n",
    "        #x = x / inv.sum(1).unsqueeze(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Dt-ndce4egY",
    "outputId": "e7244420-5c1b-4daa-fdd1-a2a919acb865"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name, max_len):\n",
    "        super(TextEncoder, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        _ = self.tokenizer.add_tokens(['[NAME]', '[RELIGION]', '[SEP]'], special_tokens=True)\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.encoder.resize_token_embeddings(len(self.tokenizer))\n",
    "        #self.encoder = text_enc\n",
    " \n",
    "    def forward(self, sentences):\n",
    "\n",
    "        x = self.tokenizer(sentences, padding='max_length', truncation=True, return_tensors='pt', max_length=self.max_len)\n",
    "        x = {\n",
    "            \"input_ids\":x[\"input_ids\"].to(0),\n",
    "            \"attention_mask\":x[\"attention_mask\"].to(0)\n",
    "        }\n",
    "        x = self.encoder(**x)[0]\n",
    "        x = x[:, 0, :]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "DFaFOTV46ukK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_audio(path):\n",
    "    wavform, _ = torchaudio.load(path)\n",
    "    output = torch.mean(wavform, dim=0)\n",
    "    return np.array(output, dtype=float)\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    '''\n",
    "    model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
    "\n",
    "    # compute attention masks and normalize the waveform if needed\n",
    "    inputs = feature_extractor(dataset[:4][\"speech\"], sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    logits = model(**inputs).logits\n",
    "    '''\n",
    "    def __init__(self, model_name):\n",
    "        super(AudioEncoder, self).__init__()\n",
    "        self.encoder = Wav2Vec2ForSequenceClassification.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "        self.audio_proj = nn.Sequential(nn.Linear(768, 1024), nn.GELU(), nn.Linear(1024, 1024))\n",
    "        self.proj = nn.Linear(1024, 128)\n",
    " \n",
    "    def forward(self, audio_paths):\n",
    "        \n",
    "        audios = list(map(load_audio, audio_paths))\n",
    "        \n",
    "        inputs = self.feature_extractor(audios, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "        \n",
    "        input_values = inputs[\"input_values\"].to(0)\n",
    "        x = torch.mean(self.encoder(input_values=input_values).hidden_states[-1], dim=1)\n",
    "        x = F.normalize(self.audio_proj(x), dim=-1)\n",
    "        x = F.normalize(self.proj(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe3Ck1R-GF2g"
   },
   "source": [
    "## MFCC Extractor and KMeans Hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "4hrurRZvfykX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import math\n",
    "\n",
    "def get_feats(x, sr):\n",
    "    #x = torch.from_numpy(x).float()\n",
    "    x = x.view(1, -1)\n",
    "\n",
    "    mfccs = torchaudio.compliance.kaldi.mfcc(\n",
    "        waveform=x,\n",
    "        sample_frequency=sr,\n",
    "        use_energy=True,\n",
    "        #subtract_mean=True,\n",
    "        #dither=1.\n",
    "    )  # (time, freq)\n",
    "    mfccs = mfccs.transpose(0, 1)  # (freq, time)\n",
    "    deltas = torchaudio.functional.compute_deltas(mfccs)\n",
    "    ddeltas = torchaudio.functional.compute_deltas(deltas)\n",
    "    concat = torch.cat([mfccs, deltas, ddeltas], dim=0)\n",
    "    concat = concat.transpose(0, 1).contiguous()  \n",
    "    return concat[:, 1:]\n",
    "\n",
    "def mfcc_feature_extractor(path, desired_sr=16000):\n",
    "    with torch.no_grad():\n",
    "        waveform, sample_rate = torchaudio.load(path, normalize=True, channels_first=True)\n",
    "        waveform = waveform.float()\n",
    "        #print(waveform)\n",
    "        if len(waveform.shape) == 2:\n",
    "            waveform = torch.mean(waveform, dim=0).unsqueeze(dim=0)\n",
    "\n",
    "        if sample_rate != desired_sr:\n",
    "            transform = torchaudio.transforms.Resample(sample_rate, desired_sr)\n",
    "            waveform = transform(waveform)\n",
    "\n",
    "        mfcc = get_feats(waveform, desired_sr)\n",
    "        return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "FclICR0xGcaV"
   },
   "outputs": [],
   "source": [
    "def get_data_cluster(path):\n",
    "    mfcc_audio = mfcc_feature_extractor(path)\n",
    "    return mfcc_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zERIAoGKOUH",
    "outputId": "62269105-5a5b-4b13-aaf0-5df31893f725"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-20.3169,   7.5364,  11.0334,  ...,   0.3019,  -1.0522,  -0.2722],\n",
       "        [-19.8434,   6.8321,   7.9347,  ...,   0.6986,  -1.3757,   0.0576],\n",
       "        [-21.2209,   6.7341,   6.1162,  ...,   1.5630,  -0.9881,   0.2393],\n",
       "        ...,\n",
       "        [ -9.6445,  -8.1216, -13.7693,  ...,  -0.6392,  -0.1196,  -0.8751],\n",
       "        [-12.8456, -13.2523, -16.0803,  ...,  -0.9978,  -0.0610,  -0.9619],\n",
       "        [ -8.9853,  -7.8230, -11.0779,  ...,  -0.2513,  -0.1463,  -0.7392]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_cluster(\"./audio/audio_emo/tess.woman.sad.6.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "w6tSMSzsGzrC"
   },
   "outputs": [],
   "source": [
    "X = torch.cat(list(map(get_data_cluster, df_train_audio[\"path\"])), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.subtract(mfcc_feat,np.mean(mfcc_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.mean(X)\n",
    "std = torch.std(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - mean)/(std+1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8176936, 38])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "M9tVGFOAHQFl"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "N_CLUSTERS = 100\n",
    "kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS,\n",
    "                          random_state=0,\n",
    "                          batch_size=300_000,\n",
    "                          max_iter=1000000,\n",
    "                          n_init=1,\n",
    "                          init=\"k-means++\") #.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SAVE = \"./pre_trained_text_encoder_6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {PATH_TO_SAVE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "IYck87IAcPFh"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(kmeans, open(f\"{PATH_TO_SAVE}/kmeans_{N_CLUSTERS}_clusters_curr.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFM4nzqrTClU"
   },
   "source": [
    "## Add mask to Transformer, try learned positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "qcY4vbMgKBHD"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class AudioEncoderMFCCHU(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 clusterization_model, \n",
    "                 emb_size=N_CLUSTERS+1, \n",
    "                 pad_idx=N_CLUSTERS, \n",
    "                 emb_dim=768, \n",
    "                 n_layers=6, \n",
    "                 padd_trunk=300, \n",
    "                 nheads=8, \n",
    "                 dropout=0.1, \n",
    "                 cluster_data_mean=mean, \n",
    "                 cluster_data_std=std):\n",
    "        super(AudioEncoderMFCCHU, self).__init__()\n",
    "        \n",
    "        self.cluster_data_mean = cluster_data_mean\n",
    "        self.cluster_data_std = cluster_data_std\n",
    "        self.clusterization_model = clusterization_model\n",
    "        self.embedding = nn.Embedding(emb_size, emb_dim, max_norm=True, padding_idx=pad_idx)\n",
    "        self.emb_size = emb_size\n",
    "        self.padd_trunk = padd_trunk\n",
    "        self.pad_idx = pad_idx\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim, dropout)\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.transf_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nheads)\n",
    "        self.transf_enc = nn.TransformerEncoder(self.transf_layer, num_layers=n_layers, norm=nn.LayerNorm(emb_dim))\n",
    "\n",
    "    def preprocess_audio(self, path):\n",
    "        mfcc = (mfcc_feature_extractor(path) - self.cluster_data_mean) / (self.cluster_data_std + 1e-10)\n",
    "        token_ids = torch.Tensor(self.clusterization_model.predict(mfcc)).long().to(0)\n",
    "        if len(token_ids) >= self.padd_trunk:\n",
    "            token_ids = token_ids[:self.padd_trunk]\n",
    "        else:\n",
    "            repeat = torch.Tensor([self.pad_idx]*(self.padd_trunk-len(token_ids))).long().to(0)\n",
    "            token_ids = torch.cat([token_ids,repeat], dim=0)\n",
    "        \n",
    "        return token_ids.unsqueeze(dim=0)\n",
    "\n",
    "    def forward(self, audio_paths):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tokens = list(map(self.preprocess_audio, audio_paths))\n",
    "            tks_tensor = torch.cat(tokens, axis=0) #.to(0)\n",
    "            assert len(tks_tensor) == len(audio_paths)\n",
    "        \n",
    "        tks_tensor_lens = 1/torch.sum(tks_tensor != self.pad_idx, dim=-1)\n",
    "        tks_tensor_lens = tks_tensor_lens.unsqueeze(dim=0).T\n",
    "\n",
    "        emb = self.embedding(tks_tensor) * math.sqrt(self.emb_dim)\n",
    "        emb = self.pos_encoder(emb)\n",
    "\n",
    "        x = self.transf_enc(emb)\n",
    "        x = tks_tensor_lens*torch.sum(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTextCLIP(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 text_encoder, \n",
    "                 audio_encoder, \n",
    "                 freeze_text_enc=False, \n",
    "                 freeze_audio_enc=False, \n",
    "                 in_features_text=384, \n",
    "                 in_features_audio=16, \n",
    "                 wide_proj=1024, \n",
    "                 proj_size=128, \n",
    "                 rate=0.1, \n",
    "                 hidden_size=384, \n",
    "                 num_classes=7, \n",
    "                 use_graph_aug=False, \n",
    "                 k=32):\n",
    "        super(AudioTextCLIP, self).__init__()\n",
    "\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "        #self.mods_proj = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.GELU(), nn.Dropout(p=rate), nn.Linear(hidden_size, hidden_size))\n",
    "        self.mods_proj = nn.Linear(hidden_size, wide_proj)\n",
    "        \n",
    "        self.text_proj = nn.Sequential(self.text_encoder, nn.Linear(in_features_text, hidden_size),  nn.GELU(), nn.Dropout(p=rate), nn.Linear(hidden_size, hidden_size), nn.GELU(), self.mods_proj)\n",
    "        \n",
    "        #self.linear1 = nn.Linear(hidden_size, wide_proj)\n",
    "        self.linear1 = lambda x: x\n",
    "        #self.linear2 = nn.Sequential(nn.Linear(wide_proj, wide_proj), nn.GELU(), nn.Linear(wide_proj, proj_size))\n",
    "        self.linear2 = nn.Linear(wide_proj, proj_size)\n",
    "        self.rate = rate\n",
    "        \n",
    "    def forward(self, inp):\n",
    "\n",
    "        sentences, audio_paths, multimodal = inp\n",
    "        \n",
    "        assert sentences != None or audio_paths != None or multimodal != None\n",
    "        \n",
    "        x = self.text_proj(sentences)\n",
    "        \n",
    "        # View 1\n",
    "        clf_emb = self.linear1(x)\n",
    "        x = F.normalize(clf_emb, dim=-1)\n",
    "        x = F.normalize(self.linear2(x), dim=-1)\n",
    "        \n",
    "        return x, clf_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkxAazkcg1wU",
    "outputId": "05e3d08b-01e8-4434-ca81-4b5601ce81dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43410 25212 5427 5095\n"
     ]
    }
   ],
   "source": [
    "print(len(train_text), len(df_train_audio), len(test_text), len(df_dev_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jmTJQwJnKCpm",
    "outputId": "98fda290-0e16-496e-a569-37b3cca3af14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'sadness', 'anger', 'joy', 'surprise', 'fear',\n",
       "       'disgust'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_audio['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "zBvA4nZwFIKV"
   },
   "outputs": [],
   "source": [
    "train_audio_repeated = pd.concat([df_train_audio, df_train_audio,df_train_audio,df_train_audio,df_train_audio,df_train_audio,df_train_audio, df_train_audio,df_train_audio,df_train_audio,df_train_audio,df_train_audio], axis=0).sample(frac=1.0).reset_index(drop=True)\n",
    "test_audio_repeated = pd.concat([df_dev_audio, df_dev_audio,df_dev_audio,df_dev_audio,df_dev_audio,df_dev_audio], axis=0).sample(frac=1.0).reset_index(drop=True)\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(torch.Tensor(list(range(len(train_text)))))\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "\n",
    "test_ds = torch.utils.data.TensorDataset(torch.Tensor(list(range(len(test_text)))))\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "mgxhTtXERPes"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I think it's a good idea to live together bef...</td>\n",
       "      <td>./audios/Ses05F_impro03_F047.wav</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>./audio/audio_emo/crema.man.sad.248.wav</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, I went over to KyleÂ’s last night to pick...</td>\n",
       "      <td>./meld_raw/dev_splits/audios/dia16_utt1.wav</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>./audio/audio_emo/crema.man.angry.193.wav</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You tell them to wait! [SEP] Okay. Wait! Wait!...</td>\n",
       "      <td>./meld_raw/dev_splits/audios/dia42_utt2.wav</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30565</th>\n",
       "      <td>That would be no. [SEP] Come on. It doesn't ta...</td>\n",
       "      <td>./meld_raw/dev_splits/audios/dia94_utt3.wav</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30566</th>\n",
       "      <td>I know</td>\n",
       "      <td>./audios/Ses05F_impro03_F028.wav</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30567</th>\n",
       "      <td>Oh no, I know! I know! It's the one where Joey...</td>\n",
       "      <td>./meld_raw/dev_splits/audios/dia28_utt8.wav</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30568</th>\n",
       "      <td>I've tried everything.  I've called everyone ...</td>\n",
       "      <td>./audios/Ses05M_impro02_M008.wav</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30569</th>\n",
       "      <td>Uh- Like, an hour and a half ago.  I talked t...</td>\n",
       "      <td>./audios/Ses05M_impro05_M002.wav</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30570 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0       I think it's a good idea to live together bef...   \n",
       "1                                                   None   \n",
       "2      Well, I went over to KyleÂ’s last night to pick...   \n",
       "3                                                   None   \n",
       "4      You tell them to wait! [SEP] Okay. Wait! Wait!...   \n",
       "...                                                  ...   \n",
       "30565  That would be no. [SEP] Come on. It doesn't ta...   \n",
       "30566                                             I know   \n",
       "30567  Oh no, I know! I know! It's the one where Joey...   \n",
       "30568   I've tried everything.  I've called everyone ...   \n",
       "30569   Uh- Like, an hour and a half ago.  I talked t...   \n",
       "\n",
       "                                              path     label  \n",
       "0                 ./audios/Ses05F_impro03_F047.wav       joy  \n",
       "1          ./audio/audio_emo/crema.man.sad.248.wav   sadness  \n",
       "2      ./meld_raw/dev_splits/audios/dia16_utt1.wav   sadness  \n",
       "3        ./audio/audio_emo/crema.man.angry.193.wav     anger  \n",
       "4      ./meld_raw/dev_splits/audios/dia42_utt2.wav   neutral  \n",
       "...                                            ...       ...  \n",
       "30565  ./meld_raw/dev_splits/audios/dia94_utt3.wav   neutral  \n",
       "30566             ./audios/Ses05F_impro03_F028.wav       joy  \n",
       "30567  ./meld_raw/dev_splits/audios/dia28_utt8.wav  surprise  \n",
       "30568             ./audios/Ses05M_impro02_M008.wav   sadness  \n",
       "30569             ./audios/Ses05M_impro05_M002.wav   neutral  \n",
       "\n",
       "[30570 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_audio_repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "hRI8ck7D7BzB"
   },
   "outputs": [],
   "source": [
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "b32SE_K1MH38"
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import gc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0ogRQkVoSv8",
    "outputId": "3da4fdbf-b45e-44b8-cff5-2c9b857cdf99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "#!pip install faiss-cpu --no-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo apt install libomp-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "9w-KtELjM3BN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "\n",
    "class FaissKNeighbors:\n",
    "    def __init__(self, k=5):\n",
    "        self.index = None\n",
    "        self.y = None\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.index = faiss.IndexFlatL2(X.shape[1])\n",
    "        self.index.add(X.astype(np.float32))\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances, indices = self.index.search(X.astype(np.float32), k=self.k)\n",
    "        votes = self.y[indices]\n",
    "        predictions = np.array([np.argmax(np.bincount(x)) for x in votes])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "#opt.zero_grad(set_to_none=True)\n",
    "#del opt\n",
    "gc.collect()\n",
    "#del supcon_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class Scheduler(_LRScheduler):\n",
    "    def __init__(self, \n",
    "                 optimizer: Optimizer,\n",
    "                 dim_embed: int,\n",
    "                 warmup_steps: int,\n",
    "                 last_epoch: int=-1,\n",
    "                 verbose: bool=False) -> None:\n",
    "\n",
    "        self.dim_embed = dim_embed\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_param_groups = len(optimizer.param_groups)\n",
    "\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "        \n",
    "    def get_lr(self) -> float:\n",
    "        lr = calc_lr(self._step_count, self.dim_embed, self.warmup_steps)\n",
    "        return [lr] * self.num_param_groups\n",
    "\n",
    "\n",
    "def calc_lr(step, dim_embed, warmup_steps):\n",
    "    #if step > warmup_steps:\n",
    "    #    return 5e-5\n",
    "    return dim_embed**(-0.5) * min(step**(-0.5), step * warmup_steps**(-1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "##xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH_TO_SAVE = \"pre_trained_text_encoder_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsupcon_model = AudioTextCLIP(\\n    TextEncoder('sentence-transformers/paraphrase-MiniLM-L3-v2', max_len=80),\\n    None,\\n    in_features_text=384,\\n    in_features_audio=768, \\n    hidden_size=1024,\\n    wide_proj=2048,\\n    proj_size=128, \\n    use_graph_aug=False,                   \\n    freeze_text_enc=False, \\n    freeze_audio_enc=False,\\n    k=32,\\n)\\nsupcon_model.load_state_dict(torch.load(f'{PATH_TO_SAVE}/pytorch_model_AudioTextCLIP_epoch_6.bin')['model'])\\n\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "supcon_model = AudioTextCLIP(\n",
    "    TextEncoder('sentence-transformers/paraphrase-MiniLM-L3-v2', max_len=80),\n",
    "    None,\n",
    "    in_features_text=384,\n",
    "    in_features_audio=768, \n",
    "    hidden_size=1024,\n",
    "    wide_proj=2048,\n",
    "    proj_size=128, \n",
    "    use_graph_aug=False,                   \n",
    "    freeze_text_enc=False, \n",
    "    freeze_audio_enc=False,\n",
    "    k=32,\n",
    ")\n",
    "supcon_model.load_state_dict(torch.load(f'{PATH_TO_SAVE}/pytorch_model_AudioTextCLIP_epoch_6.bin')['model'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(supcon_model.text_encoder, f'{PATH_TO_SAVE}/text_encoder_best.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5341a0d3bcaa4f44a3bc1673411b8135",
      "3fad1ce87a9f4b1baad4c056bd76d0ad",
      "3f40ded8f3884d468803b5cbb92d04af",
      "b272914175c14c5494bbd466ced2c24c",
      "7952be014e164b07ab81898a97fbc331",
      "98b25709d1f34983bf771a1bbd3dae3d",
      "bcfbcdafd7dd4d77a26037458e909fe5",
      "b47e5878d64c40519b6cb51321b90f62",
      "ef30cb62a33341cfa92780794e578fa8",
      "7506d573de6c4844bbf5f4af7947424b",
      "178de3d498054417b5f3661675d2deff",
      "ef6d4a9a190c411198829adc42a4e3c5",
      "ef74f18be5a940ba8696edb399e3b6f7",
      "87ef10ff662a4ea0994296b0c8577c3d",
      "104351e0f60c4dceacc51617d69cdef2",
      "b9b51ab0d78545739a420b73e0231b1d",
      "765d7438bce044758c9b1b818ede9ac4",
      "89cfc81fe5464ee9abb0a9a1a1b9d141",
      "9ca460fa5f584d9c88875eeb30eac76a",
      "b288324ff1fb4eeeb359d44cb1a40d12",
      "8d800f0793034ec29c37ac6e35ab3375",
      "5c5460fe4d344d9aaa3e87e41ca08164",
      "77d8310b1be34cf594a4731b9845e2a2",
      "862bfc472db84bfc8f031fd3a76e9809",
      "381e15620779478db7fcf1bd2d124c1b",
      "ea5c189c9660499f844a6fe3c90c59bf",
      "0b478c1f483e4fccbb6f56f002234dea",
      "d0fe5abc20594822bba649da177c106b",
      "70b8347a3a2c45b2b78a0117ec52e7ae",
      "9bf3f371933f4d328efa23edc61f9f46",
      "ffeda0d9845d4610aa25c47c14ad90f1",
      "671767bc077146a1aeed5607c971d138",
      "cf773a5a5a7c43d58957c19654160f6f",
      "7bd6fbe38ada40d5a18dcfa021326ea5",
      "9fbb36b544104cf2be8d7a6aa7cf28e1",
      "6b8ec460c2694c0ab4ed319f3d11e119",
      "93068874e3c3415bb750db3439d9cf6a",
      "bd3ced1d62224f7185d14373ad07cdff",
      "74dfc96b83f34e3c8403c192f423a6e3",
      "85f4be5ebdc94c7e8001a083a7c40154",
      "b6eba96636d54bbbbb87665595f9d96d",
      "c9532f4e341a4503abe2affac1f8e88e",
      "147da8015a5744b293df28397d0cb08c",
      "fa17642f8dcd425e87a7ea27c7782afd",
      "a41e0d7fd35c44bd80808903ea66cb61",
      "91c7531f525b4662844c113817a5bd10",
      "0b1501f7bf4a4b509267687b71bca3bb",
      "d860f362d07041228fb279ba07a48a75",
      "d135bd21ef514eeb95d91f8629bf7688",
      "563e4f1cc475402080aa71949a012f49",
      "ba6d485b3d364808894dc2336d9fd147",
      "c4dbc238f1304ecba571deabd74ff630",
      "154effefd76744f1847a4e90cd50adbe",
      "0f4366c0c2f74505b5268165f3ff15b5",
      "2ec467cae94e4f8fa22bc3d2bc563e85",
      "d6395a6be7334dc4a656a62fea25df3c",
      "b3df00cb8ab94fcf8b435765b8a9133b",
      "39e75864886e4a6c8ca47a53375d1e55",
      "bc1c88900f37416fa8ebec58ec0e8ff3",
      "9adf9f212e5945b58ff185047565bcf3",
      "6d00b5d46c58434cb2dedd09f975e504",
      "0e92d24aa67a4463843f09da7d592c94",
      "579642ddd5bf47e2861f1c7ae034f2a0",
      "a8f4bfce2b534d7b9a302ecb6986609d",
      "cbdc5f2631d94d399c00033043711360",
      "b6b140015ed746968a3f7bd4210ee5c5"
     ]
    },
    "id": "tton9xWfMTRv",
    "outputId": "1bbb0c70-8280-4c75-861a-69204da8f2ed",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                   | 0/170 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.20 GiB (GPU 0; 23.69 GiB total capacity; 20.73 GiB already allocated; 1.01 GiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 88\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#x = [multimodal, None, None]\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16) \u001b[38;5;28;01mas\u001b[39;00m autocast, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msdp_kernel(enable_flash\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m disable:\n\u001b[0;32m---> 88\u001b[0m     out1, wide \u001b[38;5;241m=\u001b[39m \u001b[43msupcon_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m out1\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     92\u001b[0m     loss \u001b[38;5;241m=\u001b[39m supcon_loss(out1, labels\u001b[38;5;241m=\u001b[39mtarget)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[51], line 38\u001b[0m, in \u001b[0;36mAudioTextCLIP.forward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     34\u001b[0m sentences, audio_paths, multimodal \u001b[38;5;241m=\u001b[39m inp\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sentences \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m audio_paths \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m multimodal \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# View 1\u001b[39;00m\n\u001b[1;32m     41\u001b[0m clf_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[36], line 22\u001b[0m, in \u001b[0;36mTextEncoder.forward\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(sentences, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len)\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m:x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m:x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m }\n\u001b[0;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1005\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1007\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1008\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1009\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1027\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:603\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    595\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    596\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:489\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    479\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:419\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    411\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    418\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 419\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    429\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:347\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    344\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/torch/nn/functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.20 GiB (GPU 0; 23.69 GiB total capacity; 20.73 GiB already allocated; 1.01 GiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "dim_embed=384\n",
    "\n",
    "supcon_model = AudioTextCLIP(\n",
    "    TextEncoder('sentence-transformers/all-MiniLM-L6-v2', max_len=256),\n",
    "    None,\n",
    "    in_features_text=384,\n",
    "    in_features_audio=dim_embed, \n",
    "    hidden_size=1024,\n",
    "    wide_proj=2048,\n",
    "    proj_size=128, \n",
    "    use_graph_aug=False,                   \n",
    "    freeze_text_enc=False, \n",
    "    freeze_audio_enc=False,\n",
    "    k=32,\n",
    ")\n",
    "\n",
    "supcon_loss = SupConLoss(temperature=0.1, contrast_mode='all', base_temperature=0.1)\n",
    "supcon_model.to(0)\n",
    "\n",
    "#supcon_model = torch.compile(supcon_model)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "step = 0\n",
    "e = 0\n",
    "patience = 9999\n",
    "early_stop_flag = 0\n",
    "old_f1 = -float('inf')\n",
    "\n",
    "param_optimizer = list(supcon_model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [{\n",
    "    'params':\n",
    "    [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "    'weight_decay_rate':\n",
    "    0.1\n",
    "}, {\n",
    "    'params':\n",
    "    [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "    'weight_decay_rate':\n",
    "    0.0\n",
    "}]\n",
    "\n",
    "opt = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5, betas=(0.9, 0.98), eps=1e-8)\n",
    "\n",
    "warmup_steps=600\n",
    "\n",
    "#scheduler = Scheduler(opt,dim_embed,warmup_steps)\n",
    "epochs = 9999\n",
    "nb_steps = 20\n",
    "\n",
    "#supcon_model.load_state_dict(torch.load(f'{PATH_TO_SAVE}/pytorch_model_AudioTextCLIP_epoch_23.bin')['model'])\n",
    "#scaler.load_state_dict(torch.load(f'{PATH_TO_SAVE}/pytorch_model_AudioTextCLIP_epoch_23.bin')['scaler'])\n",
    "#opt.load_state_dict(torch.load(f'{PATH_TO_SAVE}/pytorch_model_AudioTextCLIP_epoch_23.bin')['optimizer'])\n",
    "\n",
    "while e < epochs:\n",
    "    supcon_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    proj_val = []\n",
    "    targets_val = []\n",
    "\n",
    "    proj_train = []\n",
    "    targets_train = []\n",
    "\n",
    "    preds = []\n",
    "    support = []\n",
    "\n",
    "    for i, batch_indices in enumerate(tqdm(train_loader, total=len(train_loader))):\n",
    "\n",
    "        sentences = list(train_text.iloc[batch_indices[0]][\"text\"])\n",
    "\n",
    "        multimodal_or_audio = train_audio_repeated.iloc[batch_indices[0]]\n",
    "        multimodal_df = multimodal_or_audio[~multimodal_or_audio['text'].isna()]\n",
    "        multimodal = [str(t['text']) for _, t in multimodal_df.iterrows()]\n",
    "        audio_df = multimodal_or_audio[multimodal_or_audio['text'].isna()]\n",
    "        audio_paths = list(audio_df[\"path\"]) \n",
    "        \n",
    "        sentences = sentences + multimodal\n",
    "        \n",
    "        y_text, y_audio, y_mult = torch.Tensor(lab_encoder.transform(list(train_text.iloc[batch_indices[0]][\"grouped_label\"]))), torch.Tensor(lab_encoder.transform(list(audio_df[\"label\"]))), torch.Tensor(lab_encoder.transform(list(multimodal_df[\"label\"])))\n",
    "\n",
    "        target = torch.cat([y_text, y_mult])\n",
    "        #target = y_mult\n",
    "        x = [sentences, None, None]\n",
    "        #x = [multimodal, None, None]\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16) as autocast, torch.backends.cuda.sdp_kernel(enable_flash=False) as disable:\n",
    "\n",
    "            out1, wide = supcon_model(x)\n",
    "\n",
    "            out1 = out1.unsqueeze(dim=1)\n",
    "            \n",
    "            loss = supcon_loss(out1, labels=target)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(opt)\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(supcon_model.parameters(), 20.0)\n",
    "        #torch.nn.utils.clip_grad_norm_(supcon_model.text_encoder.parameters(), 0.1)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        #scheduler.step()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        proj_train.append(np.array(F.normalize(wide.detach(), dim=-1).cpu()))\n",
    "        #proj_train.append(np.array(wide.detach().cpu()))\n",
    "        targets_train.append(np.array(target.cpu()))\n",
    "\n",
    "        del out1\n",
    "        del wide\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    proj_train = np.concatenate(proj_train, axis=0)\n",
    "    targets_train = np.concatenate(targets_train, axis=0)\n",
    "    \n",
    "    \n",
    "    clf = FaissKNeighbors(k=128)\n",
    "    clf.fit(proj_train, np.array(targets_train, dtype=int))\n",
    "\n",
    "    epoch_loss = epoch_loss/len(train_loader)\n",
    "    supcon_model.eval()\n",
    "    preds_audios = []\n",
    "    preds_texts = []\n",
    "    targets_audios = []\n",
    "    targets_texts = []\n",
    "    \n",
    "    for i, batch_indices in enumerate(tqdm(test_loader, total=len(test_loader))):\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            #sentences = list(test_text.iloc[batch_indices[0]][\"text\"])\n",
    "\n",
    "            multimodal_or_audio = test_audio_repeated.iloc[batch_indices[0]]\n",
    "            multimodal_df = multimodal_or_audio[~multimodal_or_audio['text'].isna()]\n",
    "            multimodal = [str(t['text']) for _, t in multimodal_df.iterrows()]\n",
    "            audio_df = multimodal_or_audio[multimodal_or_audio['text'].isna()]\n",
    "            audio_paths = list(audio_df[\"path\"])\n",
    "            sentences = multimodal\n",
    "            y_text, y_audio, y_mult = torch.Tensor(lab_encoder.transform(list(test_text.iloc[batch_indices[0]][\"grouped_label\"]))), torch.Tensor(lab_encoder.transform(list(audio_df[\"label\"]))), torch.Tensor(lab_encoder.transform(list(multimodal_df[\"label\"])))\n",
    "\n",
    "            target = y_mult\n",
    "\n",
    "            x = [sentences, None, None]\n",
    "\n",
    "            _, wide = supcon_model(x)\n",
    "\n",
    "            wide = np.array(F.normalize(wide, dim=-1).cpu())\n",
    "            \n",
    "            pred = clf.predict(wide)\n",
    "\n",
    "            assert len(wide) == len(pred)\n",
    "\n",
    "            preds.append(pred)\n",
    "            proj_val.append(wide)\n",
    "            targets_val.append(np.array(target.cpu()))\n",
    "\n",
    "            del x, target\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    proj_val = np.concatenate(proj_val, axis=0)\n",
    "    targets_val = np.concatenate(targets_val, axis=0)\n",
    "    preds = np.array(np.concatenate(preds, axis=0))\n",
    "\n",
    "    general_f1 = f1_score(targets_val, preds, average='weighted')\n",
    "    general_acc = accuracy_score(targets_val, preds)\n",
    "    \n",
    "    general_f1_iemocap = f1_score(targets_val[:len(df_iemocap_test)], preds[:len(df_iemocap_test)], average='weighted')\n",
    "    general_acc_iemocap = accuracy_score(targets_val[:len(df_iemocap_test)], preds[:len(df_iemocap_test)])\n",
    "    \n",
    "    general_f1_meld = f1_score(targets_val[len(df_iemocap_test):], preds[len(df_iemocap_test):], average='weighted')\n",
    "    general_acc_meld = accuracy_score(targets_val[len(df_iemocap_test):], preds[len(df_iemocap_test):])\n",
    "    \n",
    "    print(f'General - KNN F1: {general_f1} Acc: {general_acc}')\n",
    "    print(f'Iemocap - KNN F1: {general_f1_iemocap} Acc: {general_acc_iemocap}')\n",
    "    print(f'Meld - KNN F1: {general_f1_meld} Acc: {general_acc_meld}')\n",
    "\n",
    "    idx = np.random.randint(len(proj_val), size=2000)\n",
    "\n",
    "    proj_val_samp = proj_val[idx, :]\n",
    "    targets_val_samp = targets_val[idx]\n",
    "    tsne = TSNE(n_components=2, learning_rate='auto', init='pca', perplexity=5).fit_transform(proj_val_samp)\n",
    "\n",
    "    sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=targets_val_samp, palette='tab10')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Epoch: {e + 1} - Train Loss: {epoch_loss}')\n",
    "    e += 1\n",
    "\n",
    "    with open(f\"{PATH_TO_SAVE}/metrics_epoch_{e}.txt\", \"w\") as f:\n",
    "        f.write(f'General - KNN F1 (macro): {general_f1} Acc: {general_acc}')\n",
    "        \n",
    "    checkpoint = {\"model\": supcon_model.state_dict(),\n",
    "              \"optimizer\": opt.state_dict(),\n",
    "              \"scaler\": scaler.state_dict()}\n",
    "    torch.save(checkpoint, f'{PATH_TO_SAVE}/pytorch_model_AudioTextCLIP_epoch_{e}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum([len(s.split(' ')) for s in sentences])/len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Text - KNN F1: 0.6542348387913128 Acc: 0.6550580431177446\n",
    "Audio - KNN F1: 0.5737952249753031 Acc: 0.5750875253362816\n",
    "General - KNN F1: 0.6162479641396962 Acc: 0.6150727842270131\n",
    "\n",
    "Epoch: 58 - Train Loss: 6.735767356136389\n",
    "\n",
    "torch.save(checkpoint, f'./transformer_1_layer_repetindo/pytorch_model_AudioTextCLIPvFinal_epoch_{e}_only_meld.bin')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#pickle.dump(kmeans, open(\"./transformer_1_layer_repetindo/kmeans_200_clusters_curr.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0IJvvdYd_vC"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IU9dQmJ4-gU"
   },
   "outputs": [],
   "source": [
    "supcon_model = AudioTextCLIP(\n",
    "    TextEncoder('sentence-transformers/paraphrase-MiniLM-L3-v2', max_len=80),\n",
    "    AudioEncoderMFCCHU(kmeans, emb_dim=dim_embed, n_layers=1, nheads=12),\n",
    "    in_features_text=384,\n",
    "    in_features_audio=dim_embed, \n",
    "    hidden_size=1024,\n",
    "    wide_proj=2048,\n",
    "    proj_size=128, \n",
    "    use_graph_aug=False,                   \n",
    "    freeze_text_enc=True, \n",
    "    freeze_audio_enc=False,\n",
    "    k=32,\n",
    ")\n",
    "\n",
    "\n",
    "supcon_model.to(0)\n",
    "\n",
    "supcon_model.load_state_dict(torch.load(f'{PATH_TO_SAVE}/pytorch_model_AudioTextCLIP_epoch_22.bin')['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_n_params(supcon_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supcon_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supcon_model.clusterization_model = pickle.load(open(f\"{PATH_TO_SAVE}/kmeans_200_clusters_curr.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FlMSn326Ws6"
   },
   "outputs": [],
   "source": [
    "test = supcon_model([[\"I Hate you, i believe you are shit!\", \"you are my best friend, love you!\"],None, None])[2]\n",
    "torch.dot(F.normalize(test[0, :], dim=0), F.normalize(test[1, :], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZTR2bjv6rwC"
   },
   "outputs": [],
   "source": [
    "test = supcon_model([[\"The best man ever, keep the good work!\", \"you are my best friend, love you!\"],None, None])[2]\n",
    "torch.dot(F.normalize(test[0, :], dim=0), F.normalize(test[1, :], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CozRdOGE7EB6"
   },
   "outputs": [],
   "source": [
    "test = supcon_model([[\"I Hate you, i believe you are shit!\", \"you should not be alive\"],None, None])[2]\n",
    "torch.dot(F.normalize(test[0, :], dim=0), F.normalize(test[1, :], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test = supcon_model([[\"you are my best friend, love you!\"],[\"./audio/audio_emo/tess.woman.sad.279.wav\"], None])[2]\n",
    "    print(torch.dot(F.normalize(test[0, :], dim=0), F.normalize(test[1, :], dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test = supcon_model([[\"I am sad because my dog died\"],[\"./audio/audio_emo/tess.woman.sad.279.wav\"], None])[2]\n",
    "    print(torch.dot(F.normalize(test[0, :], dim=0), F.normalize(test[1, :], dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test = supcon_model([[\"you are my best friend, love you!\"],[\"./audio/audio_emo/tess.woman.happy.50.wav\"], None])[2]\n",
    "    print(torch.dot(F.normalize(test[0, :], dim=0), F.normalize(test[1, :], dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test = supcon_model([None,[\"./audio/audio_emo/tess.woman.happy.50.wav\", \"./audio/audio_emo/crema.man.happy157.wav\"], None])[2]\n",
    "    print(torch.dot(F.normalize(test[0, :], dim=0), F.normalize(test[1, :], dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supcon_model.load_state_dict(torch.load('./pytorch_model_AudioTextCLIPvFinal_epoch_25_only_meld.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supcon_model.audio_encoder.clusterization_model = kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J92hem554uXS"
   },
   "outputs": [],
   "source": [
    "df_train_audio = pd.concat([df_iemocap_train[['text', 'path', 'label']], df_meld_train, train_audio.assign(text=[None for _ in range(len(train_audio))])], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIKswbRmlFZV"
   },
   "outputs": [],
   "source": [
    "#df_dev_audio = pd.concat([df_meld_dev, test_audio], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param: Select dataset for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_gbLE5k4yhp"
   },
   "outputs": [],
   "source": [
    "df_dev_audio = df_iemocap_test[['text', 'path', 'label']] # Iemocap\n",
    "#df_dev_audio = test_audio # Others (cremad, revss, tess)\n",
    "#df_dev_audio = df_meld_test # Meld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNjRZbK4xcOB",
    "outputId": "b8589e93-9a56-494f-a9f8-6e38a4672f01"
   },
   "outputs": [],
   "source": [
    "len(df_train_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYBBNw5txuTr",
    "outputId": "a9b71212-2cdb-43f7-c1d8-626bfef56dff"
   },
   "outputs": [],
   "source": [
    "len(df_dev_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M54RINMGv6I-"
   },
   "outputs": [],
   "source": [
    "aud = []\n",
    "for f in df_train_audio[\"path\"]:\n",
    "    if not exists(f):\n",
    "        aud.append(f)\n",
    "df_train_audio = df_train_audio[~df_train_audio['path'].isin(aud)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvrD-suPxfGf"
   },
   "outputs": [],
   "source": [
    "aud = []\n",
    "for f in df_dev_audio[\"path\"]:\n",
    "    if not exists(f):\n",
    "        aud.append(f)\n",
    "df_dev_audio = df_dev_audio[~df_dev_audio['path'].isin(aud)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjAzuLrExx8Y",
    "outputId": "12a588d1-7ed1-4c03-fc9a-55a40911a149"
   },
   "outputs": [],
   "source": [
    "len(df_dev_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_audio_repeated = pd.concat([df_train_audio, df_train_audio,df_train_audio,df_train_audio,df_train_audio,df_train_audio,df_train_audio, df_train_audio,df_train_audio,df_train_audio,df_train_audio,df_train_audio], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "#test_audio_repeated = pd.concat([df_dev_audio, df_dev_audio,df_dev_audio,df_dev_audio,df_dev_audio,df_dev_audio], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(torch.Tensor(list(range(len(df_train_audio)))))\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=2048, shuffle=True)\n",
    "\n",
    "test_ds = torch.utils.data.TensorDataset(torch.Tensor(list(range(len(df_dev_audio)))))\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kV86Kbla748i"
   },
   "outputs": [],
   "source": [
    "supcon_model.eval()\n",
    "\n",
    "proj_val = []\n",
    "targets_val = []\n",
    "\n",
    "proj_train = []\n",
    "targets_train = []\n",
    "\n",
    "preds = []\n",
    "support = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch_indices in enumerate(tqdm(train_loader, total=len(train_loader))):\n",
    "\n",
    "        sentences = list(train_text.iloc[batch_indices[0]][\"text\"])\n",
    "        #audio_paths = list(train_audio_repeated.iloc[batch_indices[0]][\"path\"])\n",
    "\n",
    "        multimodal_or_audio = train_audio_repeated.iloc[batch_indices[0]]\n",
    "        multimodal_df = multimodal_or_audio[~multimodal_or_audio['text'].isna()]\n",
    "        multimodal = [{'sentence': str(t['text']), 'audio_path':str(t['path'])} for _, t in multimodal_df.iterrows()]\n",
    "        audio_df = multimodal_or_audio[multimodal_or_audio['text'].isna()]\n",
    "        audio_paths = list(audio_df[\"path\"])\n",
    "\n",
    "        y_text, y_audio, y_mult = torch.Tensor(lab_encoder.transform(list(train_text.iloc[batch_indices[0]][\"grouped_label\"]))), torch.Tensor(lab_encoder.transform(list(audio_df[\"label\"]))), torch.Tensor(lab_encoder.transform(list(multimodal_df[\"label\"])))\n",
    "\n",
    "        target = torch.cat([y_text, y_audio, y_mult])\n",
    "\n",
    "        x = [sentences, audio_paths, multimodal]\n",
    "        for i, val in enumerate(x):\n",
    "            if len(val) == 0:\n",
    "                x[i] = None\n",
    "\n",
    "        #'''\n",
    "        with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16) as autocast, torch.backends.cuda.sdp_kernel(enable_flash=False) as disable:\n",
    "\n",
    "            _, _, wide = supcon_model(x)\n",
    "\n",
    "        #proj_train.append(np.array(F.normalize(wide.detach(), dim=-1).cpu()))\n",
    "        proj_train.append(np.array(wide.detach().cpu()))\n",
    "        #proj_train.append(np.array(wide.detach().cpu()))\n",
    "        targets_train.append(np.array(target.cpu()))\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    proj_train = np.concatenate(proj_train, axis=0)\n",
    "    targets_train = np.concatenate(targets_train, axis=0)\n",
    "\n",
    "    clf = FaissKNeighbors(k=128)\n",
    "    clf.fit(proj_train, np.array(targets_train, dtype=int))\n",
    "\n",
    "    preds_audios = []\n",
    "    preds_texts = []\n",
    "    targets_audios = []\n",
    "    targets_texts = []\n",
    "    \n",
    "    emb_audio = []\n",
    "    \n",
    "    for i, batch_indices in enumerate(tqdm(test_loader, total=len(test_loader))):\n",
    "\n",
    "        sentences = list(test_text.iloc[batch_indices[0]][\"text\"])\n",
    "\n",
    "        multimodal_or_audio = test_audio_repeated.iloc[batch_indices[0]]\n",
    "        multimodal_df = multimodal_or_audio[~multimodal_or_audio['text'].isna()]\n",
    "        multimodal = [{'sentence': str(t['text']), 'audio_path':str(t['path'])} for _, t in multimodal_df.iterrows()]\n",
    "        audio_df = multimodal_or_audio[multimodal_or_audio['text'].isna()]\n",
    "        audio_paths = list(audio_df[\"path\"])\n",
    "\n",
    "        y_text, y_audio, y_mult = torch.Tensor(lab_encoder.transform(list(test_text.iloc[batch_indices[0]][\"grouped_label\"]))), torch.Tensor(lab_encoder.transform(list(audio_df[\"label\"]))), torch.Tensor(lab_encoder.transform(list(multimodal_df[\"label\"])))\n",
    "\n",
    "        target = torch.cat([y_text, y_audio, y_mult])\n",
    "\n",
    "        x = [sentences, audio_paths, multimodal]\n",
    "        for i, val in enumerate(x):\n",
    "            if len(val) == 0:\n",
    "                x[i] = None\n",
    "        _, _, wide = supcon_model(x)\n",
    "\n",
    "        #wide = np.array(F.normalize(wide, dim=-1).cpu())\n",
    "        wide = np.array(wide.cpu())\n",
    "\n",
    "        pred = clf.predict(wide)\n",
    "\n",
    "        preds_text = clf.predict(wide[:len(sentences)])\n",
    "        preds_audio = clf.predict(wide[len(sentences):])\n",
    "\n",
    "        assert len(wide) == len(pred)\n",
    "\n",
    "        preds.append(pred)\n",
    "        preds_audios.append(preds_audio)\n",
    "        preds_texts.append(preds_text)\n",
    "        proj_val.append(wide)\n",
    "        targets_val.append(np.array(target.cpu()))\n",
    "        \n",
    "        emb_audio.append(wide[len(sentences):])\n",
    "        \n",
    "        targets_texts.append(np.array(target.cpu())[:len(sentences)])\n",
    "        targets_audios.append(np.array(target.cpu())[len(sentences):])\n",
    "\n",
    "        del x, target\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    proj_val = np.concatenate(proj_val, axis=0)\n",
    "    targets_val = np.concatenate(targets_val, axis=0)\n",
    "    preds = np.array(np.concatenate(preds, axis=0))\n",
    "    emb_audio = np.array(np.concatenate(emb_audio, axis=0))\n",
    "    \n",
    "    preds_texts = np.concatenate(preds_texts, axis=0)\n",
    "    targets_texts = np.concatenate(targets_texts, axis=0)\n",
    "    preds_audios = np.concatenate(preds_audios, axis=0)\n",
    "    targets_audios = np.concatenate(targets_audios, axis=0)\n",
    "\n",
    "    audio_f1 = f1_score(targets_audios, preds_audios, average='weighted')\n",
    "    audio_acc = accuracy_score(targets_audios, preds_audios)\n",
    "\n",
    "    text_f1 = f1_score(targets_texts, preds_texts, average='macro')\n",
    "    text_acc = accuracy_score(targets_texts, preds_texts)\n",
    "\n",
    "    curr_acc = f1_score(targets_val, preds, average='macro')\n",
    "    curr_acc2 = accuracy_score(targets_val, preds)\n",
    "\n",
    "    print(f'Text - KNN F1: {text_f1} Acc: {text_acc}')\n",
    "    print(f'Audio - KNN F1: {audio_f1} Acc: {audio_acc}')\n",
    "    print(f'General - KNN F1: {curr_acc} Acc: {curr_acc2}')\n",
    "\n",
    "    idx = np.random.randint(len(proj_val), size=2000)\n",
    "\n",
    "    proj_val_samp = proj_val[idx, :]\n",
    "    targets_val_samp = targets_val[idx]\n",
    "    tsne = TSNE(n_components=2, learning_rate='auto', init='pca', perplexity=5).fit_transform(proj_val_samp)\n",
    "\n",
    "    sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=targets_val_samp, palette='tab10')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = FaissKNeighbors(k=256)\n",
    "clf.fit(proj_train, np.array(targets_train, dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(emb_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mean_test = proj_train.mean(axis=0)\n",
    "std_test = proj_train.std(axis=0)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(proj_train/np.linalg.norm(proj_train, axis=-1, keepdims=True), targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(emb_audio/np.linalg.norm(emb_audio, axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(targets_audios, preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b1501f7bf4a4b509267687b71bca3bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4dbc238f1304ecba571deabd74ff630",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_154effefd76744f1847a4e90cd50adbe",
      "value": 112
     }
    },
    "0b478c1f483e4fccbb6f56f002234dea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e92d24aa67a4463843f09da7d592c94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f4366c0c2f74505b5268165f3ff15b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "104351e0f60c4dceacc51617d69cdef2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d800f0793034ec29c37ac6e35ab3375",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5c5460fe4d344d9aaa3e87e41ca08164",
      "value": " 629/629 [00:00&lt;00:00, 47.2kB/s]"
     }
    },
    "147da8015a5744b293df28397d0cb08c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "154effefd76744f1847a4e90cd50adbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "178de3d498054417b5f3661675d2deff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ec467cae94e4f8fa22bc3d2bc563e85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "381e15620779478db7fcf1bd2d124c1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bf3f371933f4d328efa23edc61f9f46",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ffeda0d9845d4610aa25c47c14ad90f1",
      "value": 231508
     }
    },
    "39e75864886e4a6c8ca47a53375d1e55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_579642ddd5bf47e2861f1c7ae034f2a0",
      "max": 69583549,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a8f4bfce2b534d7b9a302ecb6986609d",
      "value": 69583549
     }
    },
    "3f40ded8f3884d468803b5cbb92d04af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b47e5878d64c40519b6cb51321b90f62",
      "max": 314,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ef30cb62a33341cfa92780794e578fa8",
      "value": 314
     }
    },
    "3fad1ce87a9f4b1baad4c056bd76d0ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98b25709d1f34983bf771a1bbd3dae3d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bcfbcdafd7dd4d77a26037458e909fe5",
      "value": "Downloading (â€¦)okenizer_config.json: 100%"
     }
    },
    "5341a0d3bcaa4f44a3bc1673411b8135": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3fad1ce87a9f4b1baad4c056bd76d0ad",
       "IPY_MODEL_3f40ded8f3884d468803b5cbb92d04af",
       "IPY_MODEL_b272914175c14c5494bbd466ced2c24c"
      ],
      "layout": "IPY_MODEL_7952be014e164b07ab81898a97fbc331"
     }
    },
    "563e4f1cc475402080aa71949a012f49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "579642ddd5bf47e2861f1c7ae034f2a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c5460fe4d344d9aaa3e87e41ca08164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "671767bc077146a1aeed5607c971d138": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b8ec460c2694c0ab4ed319f3d11e119": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6eba96636d54bbbbb87665595f9d96d",
      "max": 466248,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9532f4e341a4503abe2affac1f8e88e",
      "value": 466248
     }
    },
    "6d00b5d46c58434cb2dedd09f975e504": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70b8347a3a2c45b2b78a0117ec52e7ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "74dfc96b83f34e3c8403c192f423a6e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7506d573de6c4844bbf5f4af7947424b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "765d7438bce044758c9b1b818ede9ac4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77d8310b1be34cf594a4731b9845e2a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_862bfc472db84bfc8f031fd3a76e9809",
       "IPY_MODEL_381e15620779478db7fcf1bd2d124c1b",
       "IPY_MODEL_ea5c189c9660499f844a6fe3c90c59bf"
      ],
      "layout": "IPY_MODEL_0b478c1f483e4fccbb6f56f002234dea"
     }
    },
    "7952be014e164b07ab81898a97fbc331": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bd6fbe38ada40d5a18dcfa021326ea5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9fbb36b544104cf2be8d7a6aa7cf28e1",
       "IPY_MODEL_6b8ec460c2694c0ab4ed319f3d11e119",
       "IPY_MODEL_93068874e3c3415bb750db3439d9cf6a"
      ],
      "layout": "IPY_MODEL_bd3ced1d62224f7185d14373ad07cdff"
     }
    },
    "85f4be5ebdc94c7e8001a083a7c40154": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "862bfc472db84bfc8f031fd3a76e9809": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0fe5abc20594822bba649da177c106b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_70b8347a3a2c45b2b78a0117ec52e7ae",
      "value": "Downloading (â€¦)solve/main/vocab.txt: 100%"
     }
    },
    "87ef10ff662a4ea0994296b0c8577c3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ca460fa5f584d9c88875eeb30eac76a",
      "max": 629,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b288324ff1fb4eeeb359d44cb1a40d12",
      "value": 629
     }
    },
    "89cfc81fe5464ee9abb0a9a1a1b9d141": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d800f0793034ec29c37ac6e35ab3375": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91c7531f525b4662844c113817a5bd10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_563e4f1cc475402080aa71949a012f49",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ba6d485b3d364808894dc2336d9fd147",
      "value": "Downloading (â€¦)cial_tokens_map.json: 100%"
     }
    },
    "93068874e3c3415bb750db3439d9cf6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_147da8015a5744b293df28397d0cb08c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fa17642f8dcd425e87a7ea27c7782afd",
      "value": " 466k/466k [00:00&lt;00:00, 526kB/s]"
     }
    },
    "98b25709d1f34983bf771a1bbd3dae3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9adf9f212e5945b58ff185047565bcf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bf3f371933f4d328efa23edc61f9f46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ca460fa5f584d9c88875eeb30eac76a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fbb36b544104cf2be8d7a6aa7cf28e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74dfc96b83f34e3c8403c192f423a6e3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_85f4be5ebdc94c7e8001a083a7c40154",
      "value": "Downloading (â€¦)/main/tokenizer.json: 100%"
     }
    },
    "a41e0d7fd35c44bd80808903ea66cb61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91c7531f525b4662844c113817a5bd10",
       "IPY_MODEL_0b1501f7bf4a4b509267687b71bca3bb",
       "IPY_MODEL_d860f362d07041228fb279ba07a48a75"
      ],
      "layout": "IPY_MODEL_d135bd21ef514eeb95d91f8629bf7688"
     }
    },
    "a8f4bfce2b534d7b9a302ecb6986609d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b272914175c14c5494bbd466ced2c24c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7506d573de6c4844bbf5f4af7947424b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_178de3d498054417b5f3661675d2deff",
      "value": " 314/314 [00:00&lt;00:00, 17.4kB/s]"
     }
    },
    "b288324ff1fb4eeeb359d44cb1a40d12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3df00cb8ab94fcf8b435765b8a9133b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d00b5d46c58434cb2dedd09f975e504",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0e92d24aa67a4463843f09da7d592c94",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "b47e5878d64c40519b6cb51321b90f62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6b140015ed746968a3f7bd4210ee5c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6eba96636d54bbbbb87665595f9d96d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9b51ab0d78545739a420b73e0231b1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba6d485b3d364808894dc2336d9fd147": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc1c88900f37416fa8ebec58ec0e8ff3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbdc5f2631d94d399c00033043711360",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b6b140015ed746968a3f7bd4210ee5c5",
      "value": " 69.6M/69.6M [00:00&lt;00:00, 165MB/s]"
     }
    },
    "bcfbcdafd7dd4d77a26037458e909fe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd3ced1d62224f7185d14373ad07cdff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4dbc238f1304ecba571deabd74ff630": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9532f4e341a4503abe2affac1f8e88e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cbdc5f2631d94d399c00033043711360": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf773a5a5a7c43d58957c19654160f6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0fe5abc20594822bba649da177c106b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d135bd21ef514eeb95d91f8629bf7688": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6395a6be7334dc4a656a62fea25df3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b3df00cb8ab94fcf8b435765b8a9133b",
       "IPY_MODEL_39e75864886e4a6c8ca47a53375d1e55",
       "IPY_MODEL_bc1c88900f37416fa8ebec58ec0e8ff3"
      ],
      "layout": "IPY_MODEL_9adf9f212e5945b58ff185047565bcf3"
     }
    },
    "d860f362d07041228fb279ba07a48a75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f4366c0c2f74505b5268165f3ff15b5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2ec467cae94e4f8fa22bc3d2bc563e85",
      "value": " 112/112 [00:00&lt;00:00, 7.45kB/s]"
     }
    },
    "ea5c189c9660499f844a6fe3c90c59bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_671767bc077146a1aeed5607c971d138",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_cf773a5a5a7c43d58957c19654160f6f",
      "value": " 232k/232k [00:00&lt;00:00, 341kB/s]"
     }
    },
    "ef30cb62a33341cfa92780794e578fa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ef6d4a9a190c411198829adc42a4e3c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ef74f18be5a940ba8696edb399e3b6f7",
       "IPY_MODEL_87ef10ff662a4ea0994296b0c8577c3d",
       "IPY_MODEL_104351e0f60c4dceacc51617d69cdef2"
      ],
      "layout": "IPY_MODEL_b9b51ab0d78545739a420b73e0231b1d"
     }
    },
    "ef74f18be5a940ba8696edb399e3b6f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_765d7438bce044758c9b1b818ede9ac4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_89cfc81fe5464ee9abb0a9a1a1b9d141",
      "value": "Downloading (â€¦)lve/main/config.json: 100%"
     }
    },
    "fa17642f8dcd425e87a7ea27c7782afd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ffeda0d9845d4610aa25c47c14ad90f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
