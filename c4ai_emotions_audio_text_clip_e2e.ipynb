{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DWzS3wXGtWRT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import random\n",
    "import torchaudio\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Evqqe4pG8k3z",
    "outputId": "b9003d1d-4fba-40f4-aa63-58deb401a5c8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 13 16:58:06 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3090         Off| 00000000:01:00.0  On |                  N/A |\r\n",
      "| 80%   59C    P0              154W / 350W|    668MiB / 24576MiB |      5%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      2773      G   /usr/lib/xorg/Xorg                          337MiB |\r\n",
      "|    0   N/A  N/A      2876      G   /usr/bin/gnome-shell                         82MiB |\r\n",
      "|    0   N/A  N/A      3290      G   easyeffects                                   8MiB |\r\n",
      "|    0   N/A  N/A      5958      G   ...6433698,17975664319724497082,262144      164MiB |\r\n",
      "|    0   N/A  N/A      8467      G   /usr/bin/nvidia-settings                      0MiB |\r\n",
      "|    0   N/A  N/A      9287      G   gnome-control-center                         32MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ni3W9xIGXG1Z"
   },
   "source": [
    "## Load GoEmotions and General Audio Datasets (CREMA, TESS,  RAVDASS, ETC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VJWwfnsutyBN"
   },
   "outputs": [],
   "source": [
    "train_audio = pkl.load(open('./data/c4ai_clip/train_audio.pkl', \"rb\"))[['path', 'label']]\n",
    "test_audio = pkl.load(open('./data/c4ai_clip/test_audio.pkl', \"rb\"))[['path', 'label']]\n",
    "train_text = pkl.load(open('./data/c4ai_clip/train_text.pkl', \"rb\"))[['text', 'grouped_label']]\n",
    "test_text = pkl.load(open('./data/c4ai_clip/test_text.pkl', \"rb\"))[['text', 'grouped_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_emotions = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ov6R_5UhfUd7",
    "outputId": "9d35f7b5-a38e-4b6e-8be2-daf38365486e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grouped_label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>6039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>19002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>14429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>2936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>5062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text\n",
       "grouped_label       \n",
       "anger           6039\n",
       "disgust          664\n",
       "fear             705\n",
       "joy            19002\n",
       "neutral        14429\n",
       "sadness         2936\n",
       "surprise        5062"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_emotions.groupby(\"grouped_label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger = go_emotions[go_emotions[\"grouped_label\"] == \"anger\"].sample(5000, replace=False, random_state=0)\n",
    "disgust = go_emotions[go_emotions[\"grouped_label\"] == \"disgust\"].sample(4000, replace=True, random_state=0)\n",
    "fear = go_emotions[go_emotions[\"grouped_label\"] == \"fear\"].sample(4000, replace=True, random_state=0)\n",
    "joy = go_emotions[go_emotions[\"grouped_label\"] == \"joy\"].sample(5000, replace=False, random_state=0)\n",
    "neutral = go_emotions[go_emotions[\"grouped_label\"] == \"neutral\"].sample(5000, replace=False, random_state=0)\n",
    "sadness = go_emotions[go_emotions[\"grouped_label\"] == \"sadness\"].sample(2000, replace=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grouped_label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>4664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>4705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>4936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>5062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               text\n",
       "grouped_label      \n",
       "anger          5000\n",
       "disgust        4664\n",
       "fear           4705\n",
       "joy            5000\n",
       "neutral        5000\n",
       "sadness        4936\n",
       "surprise       5062"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_emotions = go_emotions[go_emotions[\"grouped_label\"] != \"anger\"]\n",
    "go_emotions = go_emotions[go_emotions[\"grouped_label\"] != \"joy\"]\n",
    "go_emotions = go_emotions[go_emotions[\"grouped_label\"] != \"neutral\"]\n",
    "go_emotions = pd.concat([go_emotions, anger, disgust, fear, joy, neutral, sadness])\n",
    "go_emotions.groupby(\"grouped_label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kwdbm_LN0wVk"
   },
   "outputs": [],
   "source": [
    "def norm_labels(x):\n",
    "    if x == \"afraid\":\n",
    "        return \"fear\"\n",
    "    elif x == \"angry\":\n",
    "        return \"anger\"\n",
    "    elif x == \"disgusted\":\n",
    "        return \"disgust\"\n",
    "    elif x == \"sad\":\n",
    "        return \"sadness\"\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XefjoUugGJe9"
   },
   "outputs": [],
   "source": [
    "train_audio[\"label\"] = train_audio[\"label\"].apply(norm_labels)\n",
    "test_audio[\"label\"] = test_audio[\"label\"].apply(norm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "pIcNuJ6sjPt8",
    "outputId": "71110be1-2732-4d72-e8fa-3023f6f9632b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>2055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>1583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          path\n",
       "label         \n",
       "anger     1863\n",
       "disgust   1863\n",
       "fear      1863\n",
       "joy       2055\n",
       "neutral   1583\n",
       "sadness   1863\n",
       "surprise   592"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([train_audio,test_audio]).groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./audio/audio_emo/crema.woman.happy.14.wav</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./audio/audio_emo/crema.man.afraid.341.wav</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./audio/audio_emo/crema.man.neutral.370.wav</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./audio/audio_emo/tess.woman.sad.350.wav</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./audio/audio_emo/crema.man.angry.448.wav</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13085</th>\n",
       "      <td>./audio/audio_emo/tess.woman.surprised.351.wav</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13086</th>\n",
       "      <td>./audio/audio_emo/ravdass.man.surprise.63.wav</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13087</th>\n",
       "      <td>./audio/audio_emo/tess.woman.surprised.26.wav</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13088</th>\n",
       "      <td>./audio/audio_emo/tess.woman.surprised.67.wav</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13089</th>\n",
       "      <td>./audio/audio_emo/ravdass.man.surprise.52.wav</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13090 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path     label\n",
       "0          ./audio/audio_emo/crema.woman.happy.14.wav       joy\n",
       "1          ./audio/audio_emo/crema.man.afraid.341.wav      fear\n",
       "2         ./audio/audio_emo/crema.man.neutral.370.wav   neutral\n",
       "3            ./audio/audio_emo/tess.woman.sad.350.wav   sadness\n",
       "4           ./audio/audio_emo/crema.man.angry.448.wav     anger\n",
       "...                                               ...       ...\n",
       "13085  ./audio/audio_emo/tess.woman.surprised.351.wav  surprise\n",
       "13086   ./audio/audio_emo/ravdass.man.surprise.63.wav  surprise\n",
       "13087   ./audio/audio_emo/tess.woman.surprised.26.wav  surprise\n",
       "13088   ./audio/audio_emo/tess.woman.surprised.67.wav  surprise\n",
       "13089   ./audio/audio_emo/ravdass.man.surprise.52.wav  surprise\n",
       "\n",
       "[13090 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_datasets = pd.concat([train_audio,test_audio]).reset_index(drop=True)\n",
    "surprise = audio_datasets[audio_datasets[\"label\"] == \"surprise\"].sample(2000, replace=True, random_state=0)\n",
    "audio_datasets = audio_datasets[audio_datasets[\"label\"] != \"surprise\"]\n",
    "audio_datasets = pd.concat([audio_datasets, surprise]).reset_index(drop=True)\n",
    "audio_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>2055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>1583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          path\n",
       "label         \n",
       "anger     1863\n",
       "disgust   1863\n",
       "fear      1863\n",
       "joy       2055\n",
       "neutral   1583\n",
       "sadness   1863\n",
       "surprise  2000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_datasets.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qk8YSzYOX4H6"
   },
   "source": [
    "## Load Meld and IEMOCAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eDL1tDN8MLvT"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only one I know still love his parents. [B...</td>\n",
       "      <td>joy</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The only one I know still love his parents. Ye...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oh it's not bad thing it's good thing. You kno...</td>\n",
       "      <td>joy</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You know it's nice here, the air is sweet. You...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You're not sorry you came? Not sorry, no.  I c...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13723</th>\n",
       "      <td>That would be no. Come on. It doesn't taste ba...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13724</th>\n",
       "      <td>Come on. It doesn't taste bad. Yeah, it's kind...</td>\n",
       "      <td>joy</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13725</th>\n",
       "      <td>Yeah, it's kinda sweet, sorta like, uh... Cant...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13726</th>\n",
       "      <td>Cantaloupe juice. Exactly. [BFR] You've tasted...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13727</th>\n",
       "      <td>Exactly. You've tasted it? You've tasted it. [...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13728 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text     label  \\\n",
       "0      The only one I know still love his parents. [B...       joy   \n",
       "1      The only one I know still love his parents. Ye...   neutral   \n",
       "2      Oh it's not bad thing it's good thing. You kno...       joy   \n",
       "3      You know it's nice here, the air is sweet. You...   sadness   \n",
       "4      You're not sorry you came? Not sorry, no.  I c...   sadness   \n",
       "...                                                  ...       ...   \n",
       "13723  That would be no. Come on. It doesn't taste ba...   neutral   \n",
       "13724  Come on. It doesn't taste bad. Yeah, it's kind...       joy   \n",
       "13725  Yeah, it's kinda sweet, sorta like, uh... Cant...   neutral   \n",
       "13726  Cantaloupe juice. Exactly. [BFR] You've tasted...  surprise   \n",
       "13727  Exactly. You've tasted it? You've tasted it. [...   neutral   \n",
       "\n",
       "                                                    path  \n",
       "0      /home/vmachado/Documents/multimodal-datasets/I...  \n",
       "1      /home/vmachado/Documents/multimodal-datasets/I...  \n",
       "2      /home/vmachado/Documents/multimodal-datasets/I...  \n",
       "3      /home/vmachado/Documents/multimodal-datasets/I...  \n",
       "4      /home/vmachado/Documents/multimodal-datasets/I...  \n",
       "...                                                  ...  \n",
       "13723  /home/vmachado/Documents/multimodal-datasets/M...  \n",
       "13724  /home/vmachado/Documents/multimodal-datasets/M...  \n",
       "13725  /home/vmachado/Documents/multimodal-datasets/M...  \n",
       "13726  /home/vmachado/Documents/multimodal-datasets/M...  \n",
       "13727  /home/vmachado/Documents/multimodal-datasets/M...  \n",
       "\n",
       "[13728 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_erc = pd.read_csv(\"train_text_df.csv\", index_col=0).rename(columns={\"utterance\":\"text\"})\n",
    "train_df_erc[\"path\"] = train_df_erc[\"path\"].apply(lambda x: '/home/vmachado/Documents/' + x)\n",
    "train_df_erc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDKLMwOQX-6Z",
    "outputId": "ba1fa968-71e1-4f41-a9c0-42367a4d7a89"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[BFR] Brian, I need help. [AFT] Babe, I don't...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brian, I need help. [BFR] Babe, I don't know w...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Babe, I don't know what to tell you.  Don't gi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I wish I had some answers for you, babe.  I me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I went to school and I got my degree.  And I g...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>Oh, it is. It isn't. [BFR] It is. [AFT] Isn't!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847</th>\n",
       "      <td>It isn't. It is. [BFR] Isn't! [AFT]</td>\n",
       "      <td>anger</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>[BFR] Yeah baby! [AFT] I’m really glad you gu...</td>\n",
       "      <td>joy</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3849</th>\n",
       "      <td>Yeah baby! [BFR] I’m really glad you guys are ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3850</th>\n",
       "      <td>Hey.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3851 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    label  \\\n",
       "0      [BFR] Brian, I need help. [AFT] Babe, I don't...  sadness   \n",
       "1     Brian, I need help. [BFR] Babe, I don't know w...  neutral   \n",
       "2     Babe, I don't know what to tell you.  Don't gi...  neutral   \n",
       "3     I wish I had some answers for you, babe.  I me...  neutral   \n",
       "4     I went to school and I got my degree.  And I g...  neutral   \n",
       "...                                                 ...      ...   \n",
       "3846     Oh, it is. It isn't. [BFR] It is. [AFT] Isn't!  neutral   \n",
       "3847               It isn't. It is. [BFR] Isn't! [AFT]     anger   \n",
       "3848   [BFR] Yeah baby! [AFT] I’m really glad you gu...      joy   \n",
       "3849  Yeah baby! [BFR] I’m really glad you guys are ...  neutral   \n",
       "3850                                               Hey.  neutral   \n",
       "\n",
       "                                                   path  \n",
       "0     /home/vmachado/Documents/multimodal-datasets/I...  \n",
       "1     /home/vmachado/Documents/multimodal-datasets/I...  \n",
       "2     /home/vmachado/Documents/multimodal-datasets/I...  \n",
       "3     /home/vmachado/Documents/multimodal-datasets/I...  \n",
       "4     /home/vmachado/Documents/multimodal-datasets/I...  \n",
       "...                                                 ...  \n",
       "3846  /home/vmachado/Documents/multimodal-datasets/M...  \n",
       "3847  /home/vmachado/Documents/multimodal-datasets/M...  \n",
       "3848  /home/vmachado/Documents/multimodal-datasets/M...  \n",
       "3849  /home/vmachado/Documents/multimodal-datasets/M...  \n",
       "3850  /home/vmachado/Documents/multimodal-datasets/M...  \n",
       "\n",
       "[3851 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_erc = pd.read_csv(\"test_text_df.csv\", index_col=0).rename(columns={\"utterance\":\"text\"})\n",
    "test_df_erc[\"path\"] = test_df_erc[\"path\"].apply(lambda x: '/home/vmachado/Documents/' + x)\n",
    "test_df_erc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[BFR] Brian, I need help. [AFT] Babe, I don't...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "      <td>iemocap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brian, I need help. [BFR] Babe, I don't know w...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "      <td>iemocap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Babe, I don't know what to tell you.  Don't gi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "      <td>iemocap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I wish I had some answers for you, babe.  I me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "      <td>iemocap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I went to school and I got my degree.  And I g...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/I...</td>\n",
       "      <td>iemocap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>Oh, it is. It isn't. [BFR] It is. [AFT] Isn't!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "      <td>meld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847</th>\n",
       "      <td>It isn't. It is. [BFR] Isn't! [AFT]</td>\n",
       "      <td>anger</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "      <td>meld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>[BFR] Yeah baby! [AFT] I’m really glad you gu...</td>\n",
       "      <td>joy</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "      <td>meld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3849</th>\n",
       "      <td>Yeah baby! [BFR] I’m really glad you guys are ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "      <td>meld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3850</th>\n",
       "      <td>Hey.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>/home/vmachado/Documents/multimodal-datasets/M...</td>\n",
       "      <td>meld</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3851 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    label  \\\n",
       "0      [BFR] Brian, I need help. [AFT] Babe, I don't...  sadness   \n",
       "1     Brian, I need help. [BFR] Babe, I don't know w...  neutral   \n",
       "2     Babe, I don't know what to tell you.  Don't gi...  neutral   \n",
       "3     I wish I had some answers for you, babe.  I me...  neutral   \n",
       "4     I went to school and I got my degree.  And I g...  neutral   \n",
       "...                                                 ...      ...   \n",
       "3846     Oh, it is. It isn't. [BFR] It is. [AFT] Isn't!  neutral   \n",
       "3847               It isn't. It is. [BFR] Isn't! [AFT]     anger   \n",
       "3848   [BFR] Yeah baby! [AFT] I’m really glad you gu...      joy   \n",
       "3849  Yeah baby! [BFR] I’m really glad you guys are ...  neutral   \n",
       "3850                                               Hey.  neutral   \n",
       "\n",
       "                                                   path   source  \n",
       "0     /home/vmachado/Documents/multimodal-datasets/I...  iemocap  \n",
       "1     /home/vmachado/Documents/multimodal-datasets/I...  iemocap  \n",
       "2     /home/vmachado/Documents/multimodal-datasets/I...  iemocap  \n",
       "3     /home/vmachado/Documents/multimodal-datasets/I...  iemocap  \n",
       "4     /home/vmachado/Documents/multimodal-datasets/I...  iemocap  \n",
       "...                                                 ...      ...  \n",
       "3846  /home/vmachado/Documents/multimodal-datasets/M...     meld  \n",
       "3847  /home/vmachado/Documents/multimodal-datasets/M...     meld  \n",
       "3848  /home/vmachado/Documents/multimodal-datasets/M...     meld  \n",
       "3849  /home/vmachado/Documents/multimodal-datasets/M...     meld  \n",
       "3850  /home/vmachado/Documents/multimodal-datasets/M...     meld  \n",
       "\n",
       "[3851 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_erc[\"source\"] = test_df_erc[\"path\"].apply(lambda x: \"meld\" if \"MELD\" in x else \"iemocap\")\n",
    "test_df_erc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iemocap</th>\n",
       "      <td>1241</td>\n",
       "      <td>1241</td>\n",
       "      <td>1241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meld</th>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text  label  path\n",
       "source                    \n",
       "iemocap  1241   1241  1241\n",
       "meld     2610   2610  2610"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_erc.groupby(\"source\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXbw47lffnBL"
   },
   "source": [
    "## Join datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3qhKJxnIv2ma"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>1954</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>2783</td>\n",
       "      <td>2783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>5804</td>\n",
       "      <td>5804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>1451</td>\n",
       "      <td>1451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>1212</td>\n",
       "      <td>1212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text  path\n",
       "label               \n",
       "anger     1954  1954\n",
       "disgust    258   258\n",
       "fear       266   266\n",
       "joy       2783  2783\n",
       "neutral   5804  5804\n",
       "sadness   1451  1451\n",
       "surprise  1212  1212"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_erc.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RJQ3Ky9xkew8"
   },
   "outputs": [],
   "source": [
    "ang = train_df_erc[train_df_erc[\"label\"] == \"anger\"].sample(3000, replace=True, random_state=0)\n",
    "disg = train_df_erc[train_df_erc[\"label\"] == \"disgust\"].sample(4700, replace=True, random_state=0)\n",
    "fear = train_df_erc[train_df_erc[\"label\"] == \"fear\"].sample(4700, replace=True, random_state=0)\n",
    "joy = train_df_erc[train_df_erc[\"label\"] == \"joy\"].sample(2300, replace=True, random_state=0)\n",
    "sadness = train_df_erc[train_df_erc[\"label\"] == \"sadness\"].sample(3500, replace=True, random_state=0)\n",
    "surprise = train_df_erc[train_df_erc[\"label\"] == \"surprise\"].sample(3800, replace=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_erc_resampled = pd.concat([train_df_erc, joy, ang, disg, fear, surprise, sadness]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNjRZbK4xcOB",
    "outputId": "b8589e93-9a56-494f-a9f8-6e38a4672f01",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>4954</td>\n",
       "      <td>4954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>4958</td>\n",
       "      <td>4958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>4966</td>\n",
       "      <td>4966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>5083</td>\n",
       "      <td>5083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>5804</td>\n",
       "      <td>5804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>4951</td>\n",
       "      <td>4951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>5012</td>\n",
       "      <td>5012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text  path\n",
       "label               \n",
       "anger     4954  4954\n",
       "disgust   4958  4958\n",
       "fear      4966  4966\n",
       "joy       5083  5083\n",
       "neutral   5804  5804\n",
       "sadness   4951  4951\n",
       "surprise  5012  5012"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_erc_resampled.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VoxPopuli + VoxCeleb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>and i i don't believe in god no religion says ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>the question because of my mother till i was f...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>from my own culture things changed i i think a...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>of god what is a creator the almighty that uh</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>i don't wanna pinpoint what exactly god is i i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>the movie while he's solving this mystery exce...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>in my backstory you know that i actually uh hi...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>and it's just high action uh uh you want you</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>you you can't stop thinking and and wondering ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7165</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>and very flattering it's you know because i gr...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7166 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  \\\n",
       "0     /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "1     /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "2     /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "3     /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "4     /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "...                                                 ...   \n",
       "7161  /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "7162  /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "7163  /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "7164  /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "7165  /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "\n",
       "                                                   text sentiment_label  \n",
       "0     and i i don't believe in god no religion says ...         Neutral  \n",
       "1     the question because of my mother till i was f...         Neutral  \n",
       "2     from my own culture things changed i i think a...         Neutral  \n",
       "3         of god what is a creator the almighty that uh         Neutral  \n",
       "4     i don't wanna pinpoint what exactly god is i i...         Neutral  \n",
       "...                                                 ...             ...  \n",
       "7161  the movie while he's solving this mystery exce...         Neutral  \n",
       "7162  in my backstory you know that i actually uh hi...         Neutral  \n",
       "7163       and it's just high action uh uh you want you         Neutral  \n",
       "7164  you you can't stop thinking and and wondering ...         Neutral  \n",
       "7165  and very flattering it's you know because i gr...         Neutral  \n",
       "\n",
       "[7166 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vox = pd.read_csv(\"voxceleb.csv\").drop(columns=\"Unnamed: 0\")[[\"path\", \"text\", \"sentiment_label\"]]\n",
    "df_vox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>and i i don't believe in god no religion says ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>the question because of my mother till i was f...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>from my own culture things changed i i think a...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>of god what is a creator the almighty that uh</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>i don't wanna pinpoint what exactly god is i i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>the movie while he's solving this mystery exce...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>in my backstory you know that i actually uh hi...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>and it's just high action uh uh you want you</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>you you can't stop thinking and and wondering ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7165</th>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>and very flattering it's you know because i gr...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7166 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  \\\n",
       "0     /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "1     /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "2     /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "3     /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "4     /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "...                                                 ...   \n",
       "7161  /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "7162  /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "7163  /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "7164  /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "7165  /home/vmachado/.cache/huggingface/datasets/dow...   \n",
       "\n",
       "                                                   text sentiment_label  \n",
       "0     and i i don't believe in god no religion says ...         Neutral  \n",
       "1     the question because of my mother till i was f...         Neutral  \n",
       "2     from my own culture things changed i i think a...         Neutral  \n",
       "3         of god what is a creator the almighty that uh         Neutral  \n",
       "4     i don't wanna pinpoint what exactly god is i i...         Neutral  \n",
       "...                                                 ...             ...  \n",
       "7161  the movie while he's solving this mystery exce...         Neutral  \n",
       "7162  in my backstory you know that i actually uh hi...         Neutral  \n",
       "7163       and it's just high action uh uh you want you         Neutral  \n",
       "7164  you you can't stop thinking and and wondering ...         Neutral  \n",
       "7165  and very flattering it's you know because i gr...         Neutral  \n",
       "\n",
       "[7166 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ls = pd.read_csv(\"voxceleb.csv\").drop(columns=\"Unnamed: 0\")[[\"path\", \"text\", \"sentiment_label\"]] #pd.read_csv(\"df_ls.csv\")\n",
    "df_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from joblib import Parallel, delayed\n",
    "#_ = Parallel(n_jobs=12)(delayed(get_data_cluster)(x) for x in df_ls[\"path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYBBNw5txuTr",
    "outputId": "a9b71212-2cdb-43f7-c1d8-626bfef56dff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>fear</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Demographics? I don’t know anybody under 35 wh...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maybe that’s what happened to the great white ...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I never thought it was at the same moment, but...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90346</th>\n",
       "      <td>the movie while he's solving this mystery exce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90347</th>\n",
       "      <td>in my backstory you know that i actually uh hi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90348</th>\n",
       "      <td>and it's just high action uh uh you want you</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90349</th>\n",
       "      <td>you you can't stop thinking and and wondering ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90350</th>\n",
       "      <td>and very flattering it's you know because i gr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90351 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text     label  \\\n",
       "0                            To make her feel threatened      fear   \n",
       "1      OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...  surprise   \n",
       "2      Demographics? I don’t know anybody under 35 wh...  surprise   \n",
       "3      Maybe that’s what happened to the great white ...  surprise   \n",
       "4      I never thought it was at the same moment, but...  surprise   \n",
       "...                                                  ...       ...   \n",
       "90346  the movie while he's solving this mystery exce...       NaN   \n",
       "90347  in my backstory you know that i actually uh hi...       NaN   \n",
       "90348       and it's just high action uh uh you want you       NaN   \n",
       "90349  you you can't stop thinking and and wondering ...       NaN   \n",
       "90350  and very flattering it's you know because i gr...       NaN   \n",
       "\n",
       "                                                    path sentiment_label  \n",
       "0                                                   None             NaN  \n",
       "1                                                   None             NaN  \n",
       "2                                                   None             NaN  \n",
       "3                                                   None             NaN  \n",
       "4                                                   None             NaN  \n",
       "...                                                  ...             ...  \n",
       "90346  /home/vmachado/.cache/huggingface/datasets/dow...         Neutral  \n",
       "90347  /home/vmachado/.cache/huggingface/datasets/dow...         Neutral  \n",
       "90348  /home/vmachado/.cache/huggingface/datasets/dow...         Neutral  \n",
       "90349  /home/vmachado/.cache/huggingface/datasets/dow...         Neutral  \n",
       "90350  /home/vmachado/.cache/huggingface/datasets/dow...         Neutral  \n",
       "\n",
       "[90351 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat([go_emotions.rename(columns={\"grouped_label\":\"label\"}).assign(path=[None for _ in range(len(go_emotions))]), audio_datasets.assign(text=[None for _ in range(len(audio_datasets))]), train_df_erc_resampled, df_ls]).reset_index(drop=True) #.drop(columns=\"path\")\n",
    "#df_train = pd.concat([audio_datasets.assign(text=[None for _ in range(len(audio_datasets))]), train_df_erc_resampled,train_df_erc_resampled, df_ls]).reset_index(drop=True) #.drop(columns=\"path\")\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def label_to_sentiment(x):\n",
    "    if x == None:\n",
    "        return x\n",
    "    if x in [\"joy\", \"surprise\"]:\n",
    "        return \"Positive\"\n",
    "    elif x in [\"fear\", \"anger\", \"disgust\", \"sadness\"]:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"sentiment_label\"] = df_train[\"label\"].apply(label_to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>fear</td>\n",
       "      <td>None</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>None</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Demographics? I don’t know anybody under 35 wh...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>None</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maybe that’s what happened to the great white ...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>None</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I never thought it was at the same moment, but...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>None</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90346</th>\n",
       "      <td>the movie while he's solving this mystery exce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90347</th>\n",
       "      <td>in my backstory you know that i actually uh hi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90348</th>\n",
       "      <td>and it's just high action uh uh you want you</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90349</th>\n",
       "      <td>you you can't stop thinking and and wondering ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90350</th>\n",
       "      <td>and very flattering it's you know because i gr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/vmachado/.cache/huggingface/datasets/dow...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90351 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text     label  \\\n",
       "0                            To make her feel threatened      fear   \n",
       "1      OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...  surprise   \n",
       "2      Demographics? I don’t know anybody under 35 wh...  surprise   \n",
       "3      Maybe that’s what happened to the great white ...  surprise   \n",
       "4      I never thought it was at the same moment, but...  surprise   \n",
       "...                                                  ...       ...   \n",
       "90346  the movie while he's solving this mystery exce...       NaN   \n",
       "90347  in my backstory you know that i actually uh hi...       NaN   \n",
       "90348       and it's just high action uh uh you want you       NaN   \n",
       "90349  you you can't stop thinking and and wondering ...       NaN   \n",
       "90350  and very flattering it's you know because i gr...       NaN   \n",
       "\n",
       "                                                    path sentiment_label  \n",
       "0                                                   None        Negative  \n",
       "1                                                   None        Positive  \n",
       "2                                                   None        Positive  \n",
       "3                                                   None        Positive  \n",
       "4                                                   None        Positive  \n",
       "...                                                  ...             ...  \n",
       "90346  /home/vmachado/.cache/huggingface/datasets/dow...         Neutral  \n",
       "90347  /home/vmachado/.cache/huggingface/datasets/dow...         Neutral  \n",
       "90348  /home/vmachado/.cache/huggingface/datasets/dow...         Neutral  \n",
       "90349  /home/vmachado/.cache/huggingface/datasets/dow...         Neutral  \n",
       "90350  /home/vmachado/.cache/huggingface/datasets/dow...         Neutral  \n",
       "\n",
       "[90351 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative    46586\n",
       "Positive    24212\n",
       "Neutral     19553\n",
       "Name: sentiment_label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"sentiment_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "M54RINMGv6I-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lab_encoder = LabelEncoder()\n",
    "lab_encoder.fit(df_train['label'].unique())\n",
    "\n",
    "lab_encoder_senti = LabelEncoder()\n",
    "lab_encoder_senti.fit(df_train['sentiment_label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90351"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjAzuLrExx8Y",
    "outputId": "12a588d1-7ed1-4c03-fc9a-55a40911a149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3851"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df_erc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "XLc2EtfJvKuB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "HwhtSIqFvWKo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.2, contrast_mode='all',\n",
    "                 base_temperature=0.2):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None, temperature=None, base_temperature=None, weights=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        if temperature == None:\n",
    "            temperature = self.temperature\n",
    "        if base_temperature == None:\n",
    "            base_temperature = self.base_temperature\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (temperature/base_temperature) * mean_log_prob_pos\n",
    "        \n",
    "        if anchor_count == 1:\n",
    "            loss = loss.view(anchor_count, batch_size)\n",
    "        else:\n",
    "            assert anchor_count == 2 and weights != None\n",
    "            loss = loss.view(anchor_count * batch_size)\n",
    "            loss = loss * weights\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Dt-ndce4egY",
    "outputId": "e7244420-5c1b-4daa-fdd1-a2a919acb865"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name, max_len):\n",
    "        super(TextEncoder, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        _ = self.tokenizer.add_tokens(['[NAME]', '[RELIGION]', '[LAUGHTER]', '[BFR]', '[AFT]'], special_tokens=True)\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.encoder.resize_token_embeddings(len(self.tokenizer))\n",
    " \n",
    "    def forward(self, sentences):\n",
    "        \n",
    "        tokenized = self.tokenizer(sentences, padding='max_length', truncation=True, return_tensors='pt', max_length=self.max_len)\n",
    "        \n",
    "        att_mask = tokenized[\"attention_mask\"].to(0)\n",
    "        tokenized = {\n",
    "            \"input_ids\":tokenized[\"input_ids\"].to(0),\n",
    "            \"attention_mask\": att_mask\n",
    "        }\n",
    "        out = self.encoder(**tokenized)\n",
    "        \n",
    "        out = mean_pooling(out, att_mask)\n",
    "        #out = out[:, 0, :]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe3Ck1R-GF2g"
   },
   "source": [
    "## MFCC Extractor and KMeans Hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "4hrurRZvfykX"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import math\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "sample_rate = 16000\n",
    "n_fft = 400.0\n",
    "frame_length = n_fft / sample_rate * 1000.0\n",
    "frame_shift = frame_length / 2.0\n",
    "\n",
    "MFCC_PARAMS = {\n",
    "    \"channel\": 0,\n",
    "    \"dither\": 0.0,\n",
    "    \"window_type\": \"hanning\",\n",
    "    \"frame_length\": frame_length,\n",
    "    \"frame_shift\": frame_shift,\n",
    "    \"remove_dc_offset\": False,\n",
    "    \"round_to_power_of_two\": False,\n",
    "    \"sample_frequency\": sample_rate,\n",
    "}\n",
    "\n",
    "PITCH_PARAMS = {\n",
    "    \"frame_length\": frame_length,\n",
    "    \"frame_shift\": frame_shift,\n",
    "    \"sample_rate\": sample_rate,\n",
    "}\n",
    "\n",
    "def get_feats(x, sr, max_pool_window_size=4, params=MFCC_PARAMS, params_pitch=PITCH_PARAMS):\n",
    "    pitch = torchaudio.functional.compute_kaldi_pitch(x, **params_pitch).squeeze(dim=0)\n",
    "    \n",
    "    x = x.view(1, -1)\n",
    "\n",
    "    mfccs = torchaudio.compliance.kaldi.mfcc(\n",
    "        x,\n",
    "        **params\n",
    "    )  # (time, freq)\n",
    "    \n",
    "    try:\n",
    "        mfccs = torch.cat([mfccs, pitch], dim=-1)\n",
    "    except:\n",
    "        mfccs = torch.cat([mfccs, torch.Tensor(np.zeros((mfccs.shape[0], 2)))], dim=-1)\n",
    "    \n",
    "    mfccs_z = torch.Tensor(np.zeros(((mfccs.shape[0] // max_pool_window_size) + 1, mfccs.shape[1])))\n",
    "    \n",
    "    for i in range(len(mfccs) // max_pool_window_size): # Max pooling over time to reduce sequence size\n",
    "        mfcc_win = mfccs[i * max_pool_window_size:(i + 1) * max_pool_window_size]\n",
    "        norms = [np.linalg.norm(v[:-2]) for v in mfcc_win]\n",
    "        argmax = np.argmax(np.array(norms))\n",
    "        mfccs_z[i] = mfcc_win[argmax]\n",
    "            \n",
    "    mfccs = mfccs_z.transpose(0, 1)  # (freq, time)\n",
    "    deltas = torchaudio.functional.compute_deltas(mfccs)\n",
    "    ddeltas = torchaudio.functional.compute_deltas(deltas)\n",
    "    concat = torch.cat([mfccs, deltas, ddeltas], dim=0)\n",
    "    concat = concat.transpose(0, 1).contiguous()\n",
    "    \n",
    "    return concat\n",
    "\n",
    "def mfcc_feature_extractor(path, desired_sr=16000, cache=\"./preprocessed_audio_cache_new\"):\n",
    "    hashed_name = hashlib.md5(path.encode('utf-8')).hexdigest()\n",
    "    hashed_path = cache + '/' + f\"{hashed_name}.bin\"\n",
    "    with torch.no_grad():\n",
    "        if os.path.isfile(hashed_path):\n",
    "            return torch.load(hashed_path)\n",
    "        else:\n",
    "            waveform, sample_rate = torchaudio.load(path, normalize=True, channels_first=True)\n",
    "            waveform = waveform.float()\n",
    "                \n",
    "            if len(waveform.shape) == 2:\n",
    "                waveform = torch.mean(waveform, dim=0).unsqueeze(dim=0)\n",
    "\n",
    "            if sample_rate != desired_sr:\n",
    "                transform = torchaudio.transforms.Resample(sample_rate, desired_sr)\n",
    "                waveform = transform(waveform)\n",
    "\n",
    "            mfcc = get_feats(waveform, desired_sr)\n",
    "            torch.save(mfcc, hashed_path)\n",
    "            return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "FclICR0xGcaV"
   },
   "outputs": [],
   "source": [
    "def get_data_cluster(path):\n",
    "    mfcc_audio = mfcc_feature_extractor(path)\n",
    "    return mfcc_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zERIAoGKOUH",
    "outputId": "62269105-5a5b-4b13-aaf0-5df31893f725"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.4309e+01, -3.1988e+01, -7.8407e+00,  ...,  5.5061e-01,\n",
       "          3.8081e-03, -4.5475e-14],\n",
       "        [-4.4171e+01, -3.9444e+01,  5.1471e+00,  ...,  4.6405e-01,\n",
       "         -3.2662e-04, -4.5475e-14],\n",
       "        [-3.8823e+01, -5.1685e+01,  8.3763e+00,  ..., -6.5955e-01,\n",
       "          1.7061e-02, -4.5475e-14],\n",
       "        ...,\n",
       "        [-6.4814e+01, -1.1119e+01,  1.3244e+01,  ..., -9.4502e-01,\n",
       "         -4.3116e-02, -1.9203e+01],\n",
       "        [-6.6382e+01, -1.6293e+01,  1.4263e+01,  ..., -2.1900e-01,\n",
       "         -4.5776e-02, -1.4936e+01],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  5.0201e-01,\n",
       "         -9.7358e-03, -4.2674e+00]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_cluster(\"./audio/audio_emo/tess.woman.sad.5.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFM4nzqrTClU"
   },
   "source": [
    "## Add mask to Transformer, try learned positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "class AudioEncoderMFCCHUTokenizer(object):\n",
    "    \n",
    "    def __init__(self, max_length, mean, std):\n",
    "        self.max_length = max_length\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def tokenize(self, path):\n",
    "        with torch.no_grad():\n",
    "            mfcc = (mfcc_feature_extractor(path) - self.mean) / (self.std + 1e-10)\n",
    "        \n",
    "            l = len(mfcc)\n",
    "            att_mask = torch.ones((self.max_length, 1))\n",
    "\n",
    "            if l > self.max_length:\n",
    "                mfcc = mfcc[:self.max_length]\n",
    "            elif l < self.max_length:\n",
    "                mask_idx = torch.Tensor([i + l for i in range(self.max_length - l)]).long()\n",
    "                att_mask = att_mask.index_fill_(0, mask_idx, 0.0)\n",
    "                repeat = torch.zeros((self.max_length - l, mfcc.shape[1]))\n",
    "                mfcc = torch.cat([mfcc, repeat], dim=0)\n",
    "\n",
    "            mfcc = mfcc.unsqueeze(dim=0)\n",
    "            att_mask = att_mask.unsqueeze(dim=0).squeeze(dim=-1)\n",
    "            return mfcc, att_mask\n",
    "    \n",
    "    def batch_tokenize(self, paths, n_jobs=12):\n",
    "        X = Parallel(n_jobs=n_jobs)(delayed(self.tokenize)(x) for x in paths)\n",
    "        mfccs = [m for m, _ in X]\n",
    "        att_masks = [a for _, a in X]\n",
    "        mfccs = torch.cat(mfccs, dim=0)\n",
    "        att_masks = torch.cat(att_masks, dim=0)\n",
    "        return mfccs, att_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "qcY4vbMgKBHD"
   },
   "outputs": [],
   "source": [
    "from vector_quantize_pytorch import VectorQuantize, GroupedResidualVQ\n",
    "from vector_quantize_pytorch import ResidualVQ\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class AudioEncoderMFCCHU(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 emb_dim=768, \n",
    "                 n_layers=6, \n",
    "                 max_length=800,\n",
    "                 raw_features_size=45,\n",
    "                 nheads=8, \n",
    "                 dropout=0.2):\n",
    "        super(AudioEncoderMFCCHU, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.vq = ResidualVQ(\n",
    "            dim = raw_features_size,\n",
    "            codebook_size = self.vocab_size,\n",
    "            codebook_dim = 128,\n",
    "            num_quantizers = 4,\n",
    "            threshold_ema_dead_code = 2,\n",
    "        )\n",
    "            \n",
    "        self.pos_encoder = PositionalEncoding(emb_dim, dropout=0.0)\n",
    "        self.project = nn.Sequential(nn.Linear(raw_features_size, emb_dim), nn.GELU(), nn.Linear(emb_dim, emb_dim))\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dropout = dropout\n",
    "        self.transf_layer = nn.TransformerEncoderLayer(d_model=emb_dim, dim_feedforward=emb_dim*4, nhead=nheads, batch_first=True, norm_first=True, dropout=self.dropout)\n",
    "        self.transf_enc = nn.TransformerEncoder(self.transf_layer, num_layers=n_layers, norm=None)\n",
    "        \n",
    "        self.norm_feats = nn.LayerNorm(raw_features_size)\n",
    "\n",
    "    def forward(self, features, attn_masks):\n",
    "        \n",
    "        features = self.norm_feats(features)\n",
    "        qtz_feats, _, vq_loss = self.vq(features)\n",
    "        vq_loss = vq_loss.mean()\n",
    "            \n",
    "        seq_lens = 1 / torch.sum(attn_masks, dim=-1)\n",
    "        seq_lens = seq_lens.unsqueeze(dim=-1)\n",
    "\n",
    "        x = self.project(qtz_feats)\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        x = self.transf_enc(x, src_key_padding_mask=attn_masks)\n",
    "        x = seq_lens * torch.sum(x, dim=1)\n",
    "        \n",
    "        return x, vq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTextCLIP(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 text_encoder, \n",
    "                 audio_encoder, \n",
    "                 freeze_text_enc=False, \n",
    "                 freeze_audio_enc=False, \n",
    "                 in_features_text=384, \n",
    "                 in_features_audio=16, \n",
    "                 wide_proj=1024, \n",
    "                 proj_size=128,\n",
    "                 hidden_size=384,\n",
    "                 rate=0.1,):\n",
    "        super(AudioTextCLIP, self).__init__()\n",
    "        \n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "        if freeze_text_enc:\n",
    "            for i, (name, param) in enumerate(list(self.text_encoder.named_parameters())):\n",
    "                param.requires_grad = False\n",
    "            \n",
    "        if freeze_audio_enc:\n",
    "            for i, (name, param) in enumerate(list(self.audio_encoder.named_parameters())):\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.mods_proj = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.GELU(), nn.Dropout(p=rate), nn.Linear(hidden_size, wide_proj))\n",
    "        self.text_proj = nn.Sequential(self.text_encoder, nn.Linear(in_features_text, hidden_size),  nn.GELU(), nn.Dropout(p=rate), nn.Linear(hidden_size, hidden_size), nn.GELU(), self.mods_proj)\n",
    "        self.audio_proj = nn.Sequential(nn.Linear(in_features_audio, hidden_size), nn.GELU(), nn.Dropout(p=rate), nn.Linear(hidden_size, hidden_size), nn.GELU(), self.mods_proj)\n",
    "        self.linear = nn.Linear(wide_proj, proj_size)\n",
    "        \n",
    "        self.alpha = nn.Sequential(nn.Linear(wide_proj, wide_proj), nn.Dropout(p=rate), nn.Linear(wide_proj, 1))\n",
    "        \n",
    "        self.rate = rate\n",
    "        \n",
    "    def forward(self, inp):\n",
    "\n",
    "        sentences, audio_input, multimodal = inp\n",
    "        \n",
    "        assert sentences != None or audio_input != None or multimodal != None\n",
    "        \n",
    "        x_text = None\n",
    "        x_text_wide = None\n",
    "        if sentences != None:\n",
    "            x_text_wide = F.normalize(self.text_proj(sentences), dim=-1)\n",
    "            x_text = F.normalize(self.linear(x_text_wide), dim=-1)\n",
    "            \n",
    "        x_audio = None\n",
    "        x_audio_wide = None\n",
    "        if audio_input != None:\n",
    "            x_audio_wide, _ = self.audio_encoder(**audio_input)\n",
    "            x_audio_wide = F.normalize(self.audio_proj(x_audio_wide), dim=-1)\n",
    "            x_audio = F.normalize(self.linear(x_audio_wide), dim=-1)\n",
    "        \n",
    "        x_mult_text = None\n",
    "        x_mult_text_wide = None\n",
    "        x_mult_audio = None\n",
    "        x_mult_audio_wide = None\n",
    "        \n",
    "        # Approximate text and audio, and make sum of vectors point to correct cls\n",
    "        if multimodal != None:\n",
    "            x_mult_text = F.normalize(self.text_proj(multimodal['sentences']), dim=-1)\n",
    "            x_mult_text_alpha = self.alpha(x_mult_text)\n",
    "            \n",
    "            x_mult_audio, _ = self.audio_encoder(**multimodal['audio_input'])\n",
    "            x_mult_audio = F.normalize(self.audio_proj(x_mult_audio), dim=-1)\n",
    "            x_mult_audio_alpha = self.alpha(x_mult_audio)\n",
    "            \n",
    "            alphas = F.softmax(torch.cat([x_mult_text_alpha, x_mult_audio_alpha], dim=-1), dim=-1).unsqueeze(dim=1)\n",
    "\n",
    "            # View 1\n",
    "            x_mult_text_wide = alphas[:, :, 1] * x_mult_text\n",
    "            x_mult_text = alphas[:, :, 1] * F.normalize(self.linear(x_mult_text), dim=-1)\n",
    "            \n",
    "            # View 2\n",
    "            x_mult_audio_wide = alphas[:, :, 0] * x_mult_audio\n",
    "            x_mult_audio = alphas[:, :, 0] * F.normalize(self.linear(x_mult_audio), dim=-1)\n",
    "\n",
    "        return {\n",
    "            \"x_text\": x_text,\n",
    "            \"x_text_wide\": x_text_wide,\n",
    "            \"x_audio\": x_audio,\n",
    "            \"x_audio_wide\": x_audio_wide,\n",
    "            \"x_mult_text\": x_mult_text,\n",
    "            \"x_mult_text_wide\": x_mult_text_wide,\n",
    "            \"x_mult_audio\": x_mult_audio,\n",
    "            \"x_mult_audio_wide\": x_mult_audio_wide,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jmTJQwJnKCpm",
    "outputId": "98fda290-0e16-496e-a569-37b3cca3af14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fear', 'surprise', 'sadness', 'disgust', 'anger', 'joy',\n",
       "       'neutral', nan], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "b32SE_K1MH38"
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import gc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0ogRQkVoSv8",
    "outputId": "3da4fdbf-b45e-44b8-cff5-2c9b857cdf99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "9w-KtELjM3BN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "class FaissKNeighbors:\n",
    "    def __init__(self, k=5):\n",
    "        self.index = None\n",
    "        self.y = None\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.index = faiss.IndexFlatL2(X.shape[1])\n",
    "        self.index.add(X.astype(np.float32))\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances, indices = self.index.search(X.astype(np.float32), k=self.k)\n",
    "        votes = self.y[indices]\n",
    "        predictions = np.array([np.argmax(np.bincount(x)) for x in votes])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class Scheduler(_LRScheduler):\n",
    "    def __init__(self, \n",
    "                 optimizer: Optimizer,\n",
    "                 dim_embed: int,\n",
    "                 warmup_steps: int,\n",
    "                 last_epoch: int=-1,\n",
    "                 verbose: bool=False) -> None:\n",
    "\n",
    "        self.dim_embed = dim_embed\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_param_groups = len(optimizer.param_groups)\n",
    "\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "        \n",
    "    def get_lr(self) -> float:\n",
    "        lr = calc_lr(self._step_count, self.dim_embed, self.warmup_steps)\n",
    "        return [lr] * self.num_param_groups\n",
    "\n",
    "global PREVIOUS_LR\n",
    "PREVIOUS_LR = -9999\n",
    "def calc_lr(step, dim_embed, warmup_steps):\n",
    "    #if step > warmup_steps:\n",
    "    #    return 5e-5\n",
    "    global PREVIOUS_LR\n",
    "    lr = dim_embed**(-0.5) * min(step**(-0.5), step * warmup_steps**(-1.5))\n",
    "    #return lr\n",
    "    if lr < 1e-4:\n",
    "        PREVIOUS_LR = lr\n",
    "        return lr\n",
    "    else:\n",
    "        #lr = dim_embed**(-0.5) * min(step**(-0.5), step * warmup_steps**(-1.5))\n",
    "        while lr >= PREVIOUS_LR:\n",
    "            step += 1.\n",
    "            lr = dim_embed**(-0.5) * min(step**(-0.5), step * warmup_steps**(-1.5))\n",
    "        PREVIOUS_LR = lr\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embed = 768\n",
    "N_VECTORS = 100\n",
    "MAX_LEN = 128\n",
    "\n",
    "audio_encoder = AudioEncoderMFCCHU(\n",
    "    N_VECTORS, \n",
    "    emb_dim=dim_embed, \n",
    "    n_layers=3, \n",
    "    max_length=MAX_LEN, \n",
    "    nheads=8,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "#audio_encoder = torch.load(f'audio_encoder_best/audio_encoder.bin')\n",
    "#audio_encoder.load_state_dict(torch.load(f'audio_encoder_pre_trained_reformed_5_FIM_6_layer_continue/audio_best.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\n",
    "MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "text_encoder = TextEncoder(MODEL_NAME, max_len=128)\n",
    "#text_encoder.load_state_dict(torch.load('text_encoder_only_meld/dabest_text.bin'))\n",
    "#text_encoder.load_state_dict(torch.load(f'text_encoder_ready_L2_test2/best_text_encoder.bin'))\n",
    "#text_encoder.load_state_dict(torch.load(f'text_encoder_ready_L2_test2/pytorch_model_AudioTextCLIP_epoch_22.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.Tensor([-2.4002e+01, -5.4726e+00, -1.1825e+01,  6.6728e-01, -1.6782e+01,\n",
    "        -7.0169e+00, -1.4942e+01, -7.5624e+00, -7.5826e+00, -2.1841e+00,\n",
    "        -6.0805e+00, -3.0830e+00, -5.6635e+00,  1.9363e-01,  2.0110e+02,\n",
    "         7.1090e-01,  2.1245e-01,  1.8659e-01,  1.7632e-02,  3.0055e-01,\n",
    "         5.8593e-02,  2.2993e-01,  1.1659e-01,  1.2025e-01,  6.4304e-03,\n",
    "         1.1272e-01,  5.9370e-02,  1.0354e-01, -7.5894e-03, -4.8167e+00,\n",
    "         2.0838e-01,  5.0690e-02,  1.0683e-01,  1.4690e-02,  1.4356e-01,\n",
    "         6.8523e-02,  1.3284e-01,  6.2631e-02,  6.9484e-02,  3.4108e-02,\n",
    "         5.7172e-02,  4.1190e-02,  4.6194e-02, -1.4585e-03, -1.8164e+00])\n",
    "std = torch.Tensor([1.4204e+01, 1.3486e+01, 1.7198e+01, 1.7307e+01, 1.7100e+01, 1.7898e+01,\n",
    "        1.7963e+01, 1.7448e+01, 1.7087e+01, 1.7854e+01, 1.5724e+01, 1.5907e+01,\n",
    "        1.4398e+01, 4.2155e-01, 9.8201e+01, 3.8302e+00, 4.1511e+00, 4.9767e+00,\n",
    "        5.4117e+00, 5.3979e+00, 5.6093e+00, 5.7001e+00, 5.3550e+00, 5.4432e+00,\n",
    "        5.3551e+00, 4.9909e+00, 4.8445e+00, 4.4496e+00, 1.3624e-01, 1.9747e+01,\n",
    "        1.5212e+00, 1.8603e+00, 2.2115e+00, 2.4687e+00, 2.3968e+00, 2.5010e+00,\n",
    "        2.5413e+00, 2.3874e+00, 2.4448e+00, 2.3907e+00, 2.2411e+00, 2.1623e+00,\n",
    "        1.9933e+00, 6.0878e-02, 6.4246e+00])\n",
    "\n",
    "audio_tokenizer = AudioEncoderMFCCHUTokenizer(MAX_LEN, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save adio encoder again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sup_contrastive_loss(embeddings, targets, temperature=0.5):\n",
    "    batch_size = embeddings.size(0)\n",
    "    \n",
    "    mask = torch.eye(batch_size, device=embeddings.device, dtype=torch.bool)\n",
    "    \n",
    "    similarity_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n",
    "    similarity_matrix = similarity_matrix.masked_fill(mask, 0.0)\n",
    "    \n",
    "    targets = targets.contiguous().view(-1, 1)\n",
    "    targets = torch.eq(targets, targets.T).float()\n",
    "    targets = targets.masked_fill(mask, 0.0)\n",
    "    \n",
    "    loss = - (targets * F.log_softmax(similarity_matrix, dim=-1)).sum(dim=-1) / targets.sum(dim=-1)\n",
    "    \n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_contrastive_loss(embeddings1, embeddings2, temperature=0.5, weights=None):\n",
    "    assert embeddings1.size(0) == embeddings2.size(0)\n",
    "    batch_size = embeddings1.size(0)\n",
    "    embeddings = torch.cat([embeddings1, embeddings2], dim=0) # (2xN,f)\n",
    "\n",
    "    mask = torch.eye(2 * batch_size, device=embeddings.device, dtype=torch.bool)\n",
    "    \n",
    "    similarity_matrix = torch.matmul(embeddings, embeddings.T) / temperature # (2xN,2xN)\n",
    "    similarity_matrix = similarity_matrix.masked_fill(mask, 0.0)\n",
    "    \n",
    "    targets = torch.eye(batch_size, device=embeddings.device, dtype=torch.float32)\n",
    "    targets = targets.repeat(2, 2) # (2xN, 2xN) -> [[I, I], [I, I]]\n",
    "    targets = targets.masked_fill(mask, 0.0) # (2xN, 2xN) -> [[0, I], [I, 0]]\n",
    "    \n",
    "    loss = - (targets * F.log_softmax(similarity_matrix, dim=-1)).sum(dim=-1) \n",
    "    loss = loss * weights\n",
    "\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(supcon_model.audio_encoder, f'audio_encoder_best/audio_encoder.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.norm(out[\"x_mult_text\"] + out[\"x_mult_audio\"], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_train[\"sentiment_label\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mult = batch[batch[\"text\"].notna()]\n",
    "#mult = mult[mult[\"path\"].notna()].reset_index(drop=True)\n",
    "#mult[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sup_contrastive_loss(out_x_lab, target, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_x_lab @ out_x_lab.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5341a0d3bcaa4f44a3bc1673411b8135",
      "3fad1ce87a9f4b1baad4c056bd76d0ad",
      "3f40ded8f3884d468803b5cbb92d04af",
      "b272914175c14c5494bbd466ced2c24c",
      "7952be014e164b07ab81898a97fbc331",
      "98b25709d1f34983bf771a1bbd3dae3d",
      "bcfbcdafd7dd4d77a26037458e909fe5",
      "b47e5878d64c40519b6cb51321b90f62",
      "ef30cb62a33341cfa92780794e578fa8",
      "7506d573de6c4844bbf5f4af7947424b",
      "178de3d498054417b5f3661675d2deff",
      "ef6d4a9a190c411198829adc42a4e3c5",
      "ef74f18be5a940ba8696edb399e3b6f7",
      "87ef10ff662a4ea0994296b0c8577c3d",
      "104351e0f60c4dceacc51617d69cdef2",
      "b9b51ab0d78545739a420b73e0231b1d",
      "765d7438bce044758c9b1b818ede9ac4",
      "89cfc81fe5464ee9abb0a9a1a1b9d141",
      "9ca460fa5f584d9c88875eeb30eac76a",
      "b288324ff1fb4eeeb359d44cb1a40d12",
      "8d800f0793034ec29c37ac6e35ab3375",
      "5c5460fe4d344d9aaa3e87e41ca08164",
      "77d8310b1be34cf594a4731b9845e2a2",
      "862bfc472db84bfc8f031fd3a76e9809",
      "381e15620779478db7fcf1bd2d124c1b",
      "ea5c189c9660499f844a6fe3c90c59bf",
      "0b478c1f483e4fccbb6f56f002234dea",
      "d0fe5abc20594822bba649da177c106b",
      "70b8347a3a2c45b2b78a0117ec52e7ae",
      "9bf3f371933f4d328efa23edc61f9f46",
      "ffeda0d9845d4610aa25c47c14ad90f1",
      "671767bc077146a1aeed5607c971d138",
      "cf773a5a5a7c43d58957c19654160f6f",
      "7bd6fbe38ada40d5a18dcfa021326ea5",
      "9fbb36b544104cf2be8d7a6aa7cf28e1",
      "6b8ec460c2694c0ab4ed319f3d11e119",
      "93068874e3c3415bb750db3439d9cf6a",
      "bd3ced1d62224f7185d14373ad07cdff",
      "74dfc96b83f34e3c8403c192f423a6e3",
      "85f4be5ebdc94c7e8001a083a7c40154",
      "b6eba96636d54bbbbb87665595f9d96d",
      "c9532f4e341a4503abe2affac1f8e88e",
      "147da8015a5744b293df28397d0cb08c",
      "fa17642f8dcd425e87a7ea27c7782afd",
      "a41e0d7fd35c44bd80808903ea66cb61",
      "91c7531f525b4662844c113817a5bd10",
      "0b1501f7bf4a4b509267687b71bca3bb",
      "d860f362d07041228fb279ba07a48a75",
      "d135bd21ef514eeb95d91f8629bf7688",
      "563e4f1cc475402080aa71949a012f49",
      "ba6d485b3d364808894dc2336d9fd147",
      "c4dbc238f1304ecba571deabd74ff630",
      "154effefd76744f1847a4e90cd50adbe",
      "0f4366c0c2f74505b5268165f3ff15b5",
      "2ec467cae94e4f8fa22bc3d2bc563e85",
      "d6395a6be7334dc4a656a62fea25df3c",
      "b3df00cb8ab94fcf8b435765b8a9133b",
      "39e75864886e4a6c8ca47a53375d1e55",
      "bc1c88900f37416fa8ebec58ec0e8ff3",
      "9adf9f212e5945b58ff185047565bcf3",
      "6d00b5d46c58434cb2dedd09f975e504",
      "0e92d24aa67a4463843f09da7d592c94",
      "579642ddd5bf47e2861f1c7ae034f2a0",
      "a8f4bfce2b534d7b9a302ecb6986609d",
      "cbdc5f2631d94d399c00033043711360",
      "b6b140015ed746968a3f7bd4210ee5c5"
     ]
    },
    "id": "tton9xWfMTRv",
    "outputId": "1bbb0c70-8280-4c75-861a-69204da8f2ed",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/177 [00:00<?, ?it/s]/home/vmachado/anaconda3/envs/dl_ds_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "  5%|██▏                                        | 9/177 [00:16<04:29,  1.61s/it]"
     ]
    }
   ],
   "source": [
    "train_ds = torch.utils.data.TensorDataset(torch.Tensor(list(range(len(df_train)))))\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "\n",
    "test_ds = torch.utils.data.TensorDataset(torch.Tensor(list(range(len(test_df_erc)))))\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=1024, shuffle=False)\n",
    "\n",
    "PATH_TO_SAVE = 'ESTAMOS_PERTO_AMIGO_ESTOU_AQUI_4_freezed_8'\n",
    "!mkdir -p {PATH_TO_SAVE}\n",
    "\n",
    "supcon_model = AudioTextCLIP(\n",
    "    text_encoder,\n",
    "    audio_encoder,\n",
    "    in_features_text=384,\n",
    "    in_features_audio=dim_embed, \n",
    "    hidden_size=1024,\n",
    "    wide_proj=2048,\n",
    "    proj_size=128, \n",
    "    freeze_text_enc=False,\n",
    "    freeze_audio_enc=False,\n",
    "    rate=0.2,\n",
    ")\n",
    "\n",
    "# Grid search best temperatures\n",
    "# Try to only fine tune on evaluation datasets\n",
    "#supcon_model.load_state_dict(torch.load(f'ESTAMOS_PERTO_AMIGO_ESTOU_AQUI_4_freezed_4_layer/pytorch_model_AudioTextCLIP_epoch_9.bin')['model'])\n",
    "\n",
    "supcon_loss = SupConLoss(temperature=0.2, contrast_mode='all', base_temperature=0.2)\n",
    "supcon_loss_senti = SupConLoss(temperature=0.1, contrast_mode='all', base_temperature=0.1)\n",
    "supcon_loss_intra = SupConLoss(temperature=0.6, contrast_mode='all', base_temperature=0.6)\n",
    "\n",
    "supcon_model.to(0)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "step = 0\n",
    "e = 0\n",
    "patience = 9999\n",
    "early_stop_flag = 0\n",
    "old_f1 = -float('inf')\n",
    "\n",
    "param_optimizer = list(supcon_model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [{\n",
    "    'params':\n",
    "    [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "    'weight_decay_rate':\n",
    "    0.1\n",
    "}, {\n",
    "    'params':\n",
    "    [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "    'weight_decay_rate':\n",
    "    0.0\n",
    "}]\n",
    "\n",
    "scheduler_epochs = 5\n",
    "opt = torch.optim.AdamW(optimizer_grouped_parameters, lr=0, betas=(0.9, 0.98), eps=1e-8)\n",
    "#scheduler = torch.optim.lr_scheduler.LinearLR(opt, start_factor=0.5, end_factor=0.9, total_iters=10, last_epoch=- 1, verbose=False)\n",
    "scheduler = Scheduler(opt, 768, 600)\n",
    "\n",
    "epochs = 9999\n",
    "\n",
    "while e < epochs:\n",
    "    supcon_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    proj_val = []\n",
    "    targets_val = []\n",
    "\n",
    "    proj_train = []\n",
    "    targets_train = []\n",
    "\n",
    "    for i, batch_indices in enumerate(tqdm(train_loader, total=len(train_loader))):\n",
    "        if i == len(train_loader)-1:\n",
    "            continue\n",
    "        batch = df_train.iloc[batch_indices[0]]\n",
    "        only_text = batch[batch[\"path\"].isna()]\n",
    "        sentences = only_text[\"text\"].tolist()\n",
    "        y_text = torch.Tensor(lab_encoder.transform(only_text[\"label\"]))\n",
    "        y_text_senti = torch.Tensor(lab_encoder_senti.transform(only_text[\"sentiment_label\"]))\n",
    "        \n",
    "        only_audio = batch[batch[\"text\"].isna()]\n",
    "        audio_paths = only_audio[\"path\"].tolist()\n",
    "        try:\n",
    "            mfccs, att = audio_tokenizer.batch_tokenize(audio_paths)\n",
    "\n",
    "            audio_input = {\n",
    "                \"features\": mfccs.to(0),\n",
    "                \"attn_masks\": att.to(0),\n",
    "            }\n",
    "        except:\n",
    "            audio_input = None\n",
    "\n",
    "        y_audio = torch.Tensor(lab_encoder.transform(only_audio[\"label\"]))\n",
    "        y_audio_senti = torch.Tensor(lab_encoder_senti.transform(only_audio[\"sentiment_label\"]))\n",
    "        \n",
    "        mult = batch[batch[\"text\"].notna()]\n",
    "        mult = mult[mult[\"path\"].notna()].reset_index(drop=True)\n",
    "        \n",
    "        mult_not_na_idx = mult[mult[\"label\"].notna()].index\n",
    "        #batch_not_na_idx = batch[batch[\"label\"].notna()].index\n",
    "        #mult_na_idx = mult[mult[\"label\"].isna()].index\n",
    "        \n",
    "        y_mult = torch.Tensor(lab_encoder.transform(mult.iloc[mult_not_na_idx][\"label\"]))\n",
    "        \n",
    "        y_mult_senti = torch.Tensor(lab_encoder_senti.transform(mult[\"sentiment_label\"]))\n",
    "        \n",
    "        audio_path_mult = [str(t['path']) for _, t in mult.iterrows()]\n",
    "        \n",
    "        mfccs_mult, att_mult = audio_tokenizer.batch_tokenize(audio_path_mult)\n",
    "        \n",
    "        sentences_mult = [str(t['text']) for _, t in mult.iterrows()]\n",
    "        \n",
    "        multimodal = {'sentences': sentences_mult, \n",
    "                      'audio_input': {\"features\": mfccs_mult.to(0), \"attn_masks\": att_mult.to(0)}}\n",
    "        \n",
    "        target = torch.cat([y_text, y_audio, y_mult]).long().cuda()\n",
    "        target_senti = torch.cat([y_text_senti, y_audio_senti, y_mult_senti]).long().cuda()\n",
    "        \n",
    "        x = [sentences, audio_input, multimodal]\n",
    "        \n",
    "        if len(sentences) == 0:\n",
    "            x[0] = None\n",
    "        if len(audio_paths) == 0:\n",
    "            x[1] = None\n",
    "        if len(sentences_mult) == 0:\n",
    "            x[2] = None\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16) as autocast, torch.backends.cuda.sdp_kernel(enable_flash=False) as disable:\n",
    "            \n",
    "            out = supcon_model(x)\n",
    "            \n",
    "            # Multimodal loss\n",
    "            if x[-1] is not None:\n",
    "                x_mult_text = out[\"x_mult_text\"]\n",
    "                x_mult_audio = out[\"x_mult_audio\"]\n",
    "                x_mult_text_norm = F.normalize(x_mult_text, dim=-1)\n",
    "                x_mult_audio_norm = F.normalize(x_mult_audio, dim=-1)\n",
    "                \n",
    "                #weights = torch.stack([torch.norm(x_mult_audio, dim=-1), torch.norm(x_mult_text, dim=-1)], dim=0).detach()\n",
    "                weights = torch.cat([torch.norm(x_mult_audio, dim=-1), torch.norm(x_mult_text, dim=-1)], dim=0).detach()\n",
    "                #weights = torch.ones_like(weights)\n",
    "                \n",
    "                # Augument modality\n",
    "                augs = random.choices(\n",
    "                    population=[0, 1, 2],\n",
    "                    weights=[0.5, 0.25, 0.25],\n",
    "                    k=len(x_mult_text)\n",
    "                )\n",
    "                \n",
    "                x_mult = torch.stack([F.normalize(x_mult_text + x_mult_audio, dim=-1), \n",
    "                                      x_mult_text_norm, \n",
    "                                     x_mult_audio_norm], dim=1)\n",
    "\n",
    "                x_mult = x_mult[list(range(len(augs))), augs, :] \n",
    "                x_mult_wide = F.normalize(out[\"x_mult_text_wide\"] + out[\"x_mult_audio_wide\"], dim=-1)\n",
    "                \n",
    "                # Add weighted contrastive loss\n",
    "                #x_mult_text = x_mult_text_norm.unsqueeze(dim=1)\n",
    "                #x_mult_audio = x_mult_audio_norm.unsqueeze(dim=1)\n",
    "                #mult = torch.cat([x_mult_text, x_mult_audio], dim=1)\n",
    "                \n",
    "                out_x, out_x_wide = None, None\n",
    "                \n",
    "                if x[0] is not None:\n",
    "                    if x[1] is not None:\n",
    "                        out_x = torch.cat([out[\"x_text\"], out[\"x_audio\"], x_mult], dim=0) #.unsqueeze(dim=1)\n",
    "                        out_x_lab = torch.cat([out[\"x_text\"], out[\"x_audio\"], x_mult[mult_not_na_idx]], dim=0) #.unsqueeze(dim=1)\n",
    "                        out_x_wide = torch.cat([out[\"x_text_wide\"], out[\"x_audio_wide\"], x_mult_wide[mult_not_na_idx]], dim=0)\n",
    "                    else:\n",
    "                        out_x = torch.cat([out[\"x_text\"], x_mult], dim=0) #.unsqueeze(dim=1)\n",
    "                        out_x_lab = torch.cat([out[\"x_text\"], x_mult[mult_not_na_idx]], dim=0) #.unsqueeze(dim=1)\n",
    "                        out_x_wide = torch.cat([out[\"x_text_wide\"], x_mult_wide[mult_not_na_idx]], dim=0)\n",
    "                elif x[1] is not None:\n",
    "                    out_x = torch.cat([out[\"x_audio\"], x_mult], dim=0) #.unsqueeze(dim=1)\n",
    "                    out_x_lab = torch.cat([out[\"x_audio\"], x_mult[mult_not_na_idx]], dim=0) #.unsqueeze(dim=1)\n",
    "                    out_x_wide = torch.cat([out[\"x_audio_wide\"], x_mult_wide[mult_not_na_idx]], dim=0)\n",
    "                else:\n",
    "                    out_x = x_mult.unsqueeze(dim=1)\n",
    "                    out_x_lab = x_mult[mult_not_na_idx] #.unsqueeze(dim=1)\n",
    "                    out_x_wide = x_mult_wide[mult_not_na_idx]\n",
    "                \n",
    "                # fera ta\n",
    "                \n",
    "                loss = 0.8 * (0.5 * sup_contrastive_loss(out_x_lab, target, temperature=0.2) + 0.5 * sup_contrastive_loss(out_x, target_senti, temperature=0.1)) + 0.2 * unsupervised_contrastive_loss(x_mult_text_norm, x_mult_audio_norm, temperature=0.8, weights=weights)\n",
    "                #loss = 0.5 * (0.5 * supcon_loss(out_x_lab, labels=target) + 0.5 * supcon_loss_senti(out_x, labels=target_senti)) + 0.5 * supcon_loss_intra(mult, weights=weights) \n",
    "            else:\n",
    "                if x[0] is not None:\n",
    "                    if x[1] is not None:\n",
    "                        out_x = torch.cat([out[\"x_text\"], out[\"x_audio\"]], dim=0).unsqueeze(dim=1)\n",
    "                        out_x_wide = torch.cat([out[\"x_text_wide\"], out[\"x_audio_wide\"]], dim=0)\n",
    "                    else:\n",
    "                        out_x = out[\"x_text\"]\n",
    "                        out_x_wide = out[\"x_text_wide\"]\n",
    "                else:\n",
    "                    if x[1] is not None:\n",
    "                        out_x = out[\"x_audio\"]\n",
    "                        out_x_wide = out[\"x_audio_wide\"]\n",
    "                    else:\n",
    "                        raise Exception(\"Nothing to work :()\")\n",
    "                        \n",
    "                loss = supcon_loss(out_x, labels=target) + supcon_loss(out_x, labels=target_senti)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(opt)\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(supcon_model.parameters(), 20.0)\n",
    "        scaler.step(opt)\n",
    "        scheduler.step()\n",
    "        scaler.update()\n",
    "        \n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        proj_train.append(np.array(out_x_wide.detach().cpu()))\n",
    "        targets_train.append(np.array(target.cpu()))\n",
    "        if i == 20:\n",
    "            print(loss.item())\n",
    "            break\n",
    "        del out_x\n",
    "        del x_mult\n",
    "        del out_x_wide\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    #scheduler.step()\n",
    "    proj_train = np.concatenate(proj_train, axis=0)\n",
    "    targets_train = np.concatenate(targets_train, axis=0)\n",
    "    \n",
    "    clf = FaissKNeighbors(k=128)\n",
    "    clf.fit(proj_train, np.array(targets_train, dtype=int))\n",
    "\n",
    "    epoch_loss = epoch_loss/len(train_loader)\n",
    "    supcon_model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    css = 0.0\n",
    "    wide_audio = []\n",
    "    \n",
    "    for i, batch_indices in enumerate(tqdm(test_loader, total=len(test_loader))):\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            multimodal_batch = test_df_erc.iloc[batch_indices[0]]\n",
    "\n",
    "            audio_path_mult = [str(t['path']) for _, t in multimodal_batch.iterrows()]\n",
    "            mfccs_mult, att_mult = audio_tokenizer.batch_tokenize(audio_path_mult)\n",
    "\n",
    "            sentences_mult = [str(t['text']) for _, t in multimodal_batch.iterrows()]\n",
    "\n",
    "            multimodal = {'sentences': sentences_mult, \n",
    "                          'audio_input': {\"features\": mfccs_mult.to(0), \"attn_masks\": att_mult.to(0)}}\n",
    "        \n",
    "            target = torch.Tensor(lab_encoder.transform(list(multimodal_batch[\"label\"])))\n",
    "\n",
    "            x = [None, None, multimodal]\n",
    "            with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16) as autocast, torch.backends.cuda.sdp_kernel(enable_flash=False) as disable:\n",
    "                out = supcon_model(x)\n",
    "            \n",
    "            # Multimodal loss\n",
    "            out_x_wide = F.normalize(out[\"x_mult_text_wide\"] + out[\"x_mult_audio_wide\"], dim=-1)\n",
    "            #out_x_wide = F.normalize(out[\"x_mult_audio_wide\"], dim=-1)\n",
    "            \n",
    "            cs = F.cosine_similarity(F.normalize(out[\"x_mult_text_wide\"], dim=-1), F.normalize(out[\"x_mult_audio_wide\"], dim=-1))\n",
    "\n",
    "            wide = np.array(out_x_wide.cpu())\n",
    "            wide_audio.append(np.array(F.normalize(out[\"x_mult_audio_wide\"], dim=-1).cpu()))\n",
    "            pred = clf.predict(wide)\n",
    "            preds.append(pred)\n",
    "\n",
    "            assert len(wide) == len(pred)\n",
    "\n",
    "            proj_val.append(wide)\n",
    "            targets_val.append(np.array(target.cpu()))\n",
    "            css += np.sum(np.array(cs.cpu()))\n",
    "            del out_x_wide\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    proj_val = np.concatenate(proj_val, axis=0)\n",
    "    wide_audio = np.concatenate(wide_audio, axis=0)\n",
    "    targets_val = np.concatenate(targets_val, axis=0)\n",
    "    \n",
    "    preds = np.array(np.concatenate(preds, axis=0))\n",
    "    \n",
    "    css = css / len(test_df_erc)\n",
    "\n",
    "    general_f1 = f1_score(targets_val, preds, average='weighted')\n",
    "    general_acc = accuracy_score(targets_val, preds)\n",
    "    \n",
    "    print(f'Cosine Similarity between mods: {css}')\n",
    "    \n",
    "    meld_idx = test_df_erc[test_df_erc[\"source\"] == \"meld\"].index\n",
    "    iemocap_idx = test_df_erc[test_df_erc[\"source\"] != \"meld\"].index\n",
    "    \n",
    "    general_f1_iemocap = f1_score(targets_val[iemocap_idx], preds[iemocap_idx], average='weighted')\n",
    "    general_f1_iemocap_audio = f1_score(targets_val[iemocap_idx], clf.predict(wide_audio)[iemocap_idx], average='weighted')\n",
    "    general_acc_iemocap = accuracy_score(targets_val[iemocap_idx], preds[iemocap_idx])\n",
    "    \n",
    "    general_f1_meld = f1_score(targets_val[meld_idx], preds[meld_idx], average='weighted')\n",
    "    general_acc_meld = accuracy_score(targets_val[meld_idx], preds[meld_idx])\n",
    "    \n",
    "    print(f'General - KNN F1: {general_f1} Acc: {general_acc}')\n",
    "    print(f'Iemocap - KNN F1: {general_f1_iemocap} Acc: {general_acc_iemocap}')\n",
    "    print(f'Iemocap - KNN F1 - Only Audio: {general_f1_iemocap_audio}')\n",
    "    print(f'Meld - KNN F1: {general_f1_meld} Acc: {general_acc_meld}')\n",
    "    \n",
    "    print(f\"Iemocap - KNN F1 (macro): {f1_score(targets_val[iemocap_idx], preds[iemocap_idx], average='macro')}\")\n",
    "    print(f\"Meld - KNN F1 (macro): {f1_score(targets_val[meld_idx], preds[meld_idx], average='macro')}\")\n",
    "\n",
    "    try:\n",
    "        tsne = TSNE(n_components=2, learning_rate='auto', init='pca', perplexity=5).fit_transform(proj_val)\n",
    "\n",
    "        sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=lab_encoder.inverse_transform(list(np.array(targets_val, dtype=int))) , palette='tab10')\n",
    "        plt.show()\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(f'Epoch: {e + 1} - Train Loss: {epoch_loss}')\n",
    "    e += 1\n",
    "    \n",
    "    #if e == scheduler_epochs: # Unfreeze text encoder\n",
    "    #    for i, (name, param) in enumerate(list(supcon_model.text_encoder.named_parameters())):\n",
    "    #        param.requires_grad = True\n",
    "\n",
    "    with open(f\"{PATH_TO_SAVE}/metrics_epoch_{e}.txt\", \"w\") as f:\n",
    "        f.write(f'General - KNN F1: {general_f1} Acc: {general_acc}')\n",
    "        f.write(f'Iemocap - KNN F1: {general_f1_iemocap} Acc: {general_acc_iemocap}')\n",
    "        f.write(f'Meld - KNN F1: {general_f1_meld} Acc: {general_acc_meld}')\n",
    "        f.write(f\"Iemocap - KNN F1 (macro): {f1_score(targets_val[iemocap_idx], preds[iemocap_idx], average='macro')}\")\n",
    "        f.write(f\"Meld - KNN F1 (macro): {f1_score(targets_val[meld_idx], preds[meld_idx], average='macro')}\")\n",
    "        \n",
    "    checkpoint = {\"model\": supcon_model.state_dict(),\n",
    "              \"optimizer\": opt.state_dict(),\n",
    "              \"scaler\": scaler.state_dict()}\n",
    "    torch.save(checkpoint, f'{PATH_TO_SAVE}/pytorch_model_AudioTextCLIP_epoch_{e}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mult_not_na_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1608 + 3757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_x[mult_not_na_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cosine Similarity between mods: -0.13657516783951593\n",
    "General - KNN F1: 0.6451182550010343 Acc: 0.6429577464788733\n",
    "Iemocap - KNN F1: 0.7277911203409533 Acc: 0.7260606060606061\n",
    "Meld - KNN F1: 0.5947792167273063 Acc: 0.5904214559386973\n",
    "Iemocap - KNN F1 (macro): 0.5760597935589471\n",
    "Meld - KNN F1 (macro): 0.4407585720404875\n",
    "\n",
    "Cosine Similarity between mods: -0.008845211082780864\n",
    "General - KNN F1: 0.6472219462144913 Acc: 0.6460093896713615\n",
    "Iemocap - KNN F1: 0.7313628195864195 Acc: 0.7296969696969697\n",
    "Meld - KNN F1: 0.5962234730821557 Acc: 0.593103448275862\n",
    "Iemocap - KNN F1 (macro): 0.5782869917527181\n",
    "Meld - KNN F1 (macro): 0.4393689351519012\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#pickle.dump(kmeans, open(\"./transformer_1_layer_repetindo/kmeans_200_clusters_curr.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0IJvvdYd_vC"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH_TO_SAVE = 'ESTAMOS_PERTO_AMIGO_ESTOU_AQUI_4_freezed_5_layer_pivoting_to_speech_training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.load(f'pre_test_final_2/pytorch_model_AudioTextCLIP_epoch_35.bin')['model']\n",
    "#torch.load(f'{PATH_TO_SAVE}/pytorch_model_AudioTextCLIP_epoch_1.bin')['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IU9dQmJ4-gU"
   },
   "outputs": [],
   "source": [
    "\n",
    "PATH_TO_SAVE = 'ESTAMOS_PERTO_AMIGO_ESTOU_AQUI_4_freezed_7'\n",
    "\n",
    "supcon_model = AudioTextCLIP(\n",
    "    text_encoder,\n",
    "    audio_encoder,\n",
    "    in_features_text=384,\n",
    "    in_features_audio=dim_embed, \n",
    "    hidden_size=1024,\n",
    "    wide_proj=2048,\n",
    "    proj_size=128, \n",
    "    freeze_text_enc=True,\n",
    "    freeze_audio_enc=False,\n",
    "    rate=0.1,\n",
    ").to(0)\n",
    "supcon_model.load_state_dict(torch.load(f'{PATH_TO_SAVE}/pytorch_model_AudioTextCLIP_epoch_35.bin')['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_n_params(supcon_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supcon_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(test[\"x_text_wide\"][0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FlMSn326Ws6"
   },
   "outputs": [],
   "source": [
    "test = supcon_model([[\"I Hate you, i believe you are shit!\", \"You are my best friend, love you!\"],None, None])\n",
    "torch.dot(F.normalize(test[\"x_text_wide\"][0, :], dim=-1), F.normalize(test[\"x_text_wide\"][1, :], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZTR2bjv6rwC"
   },
   "outputs": [],
   "source": [
    "test = supcon_model([[\"The best man ever, keep the good work!\", \"you are my best friend, love you!\"],None, None])\n",
    "torch.dot(F.normalize(test[\"x_text_wide\"][0, :], dim=0), F.normalize(test[\"x_text_wide\"][1, :], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CozRdOGE7EB6"
   },
   "outputs": [],
   "source": [
    "test = supcon_model([[\"I Hate you, i believe you are shit!\", \"Fuck you, you should not be alive\"],None, None])\n",
    "torch.dot(F.normalize(test[\"x_text_wide\"][0, :], dim=0), F.normalize(test[\"x_text_wide\"][1, :], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    m, a = audio_tokenizer.batch_tokenize([\"./audio/audio_emo/tess.woman.sad.100.wav\"])\n",
    "    audio_input = {\n",
    "        \"features\": m.to(0),\n",
    "        \"attn_masks\": a.to(0),\n",
    "    }\n",
    "    test = supcon_model([[\"Thanks! you are my best friend, I love you!\"],audio_input, None])\n",
    "    print(torch.dot(F.normalize(test[\"x_text_wide\"][0, :], dim=0), F.normalize(test[\"x_audio_wide\"][0, :], dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    m, a = audio_tokenizer.batch_tokenize([\"./audio/audio_emo/tess.woman.sad.279.wav\"])\n",
    "    audio_input = {\n",
    "        \"features\": m.to(0),\n",
    "        \"attn_masks\": a.to(0),\n",
    "    }\n",
    "    test = supcon_model([[\"I am sad because my dog died\"], audio_input, None])\n",
    "    print(torch.dot(F.normalize(test[\"x_text_wide\"][0, :], dim=-1), F.normalize(test[\"x_audio_wide\"][0, :], dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    m, a = audio_tokenizer.batch_tokenize([\"./audio/audio_emo/tess.woman.happy.50.wav\"])\n",
    "    audio_input = {\n",
    "        \"features\": m.to(0),\n",
    "        \"attn_masks\": a.to(0),\n",
    "    }\n",
    "    test = supcon_model([[\"you are my best friend, love you!\"],audio_input, None])\n",
    "    print(torch.dot(F.normalize(test[\"x_text_wide\"][0, :], dim=0), F.normalize(test[\"x_audio_wide\"][0, :], dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    m, a = audio_tokenizer.batch_tokenize([\"./audio/audio_emo/tess.woman.happy.50.wav\", \"./audio/audio_emo/crema.man.happy157.wav\"])\n",
    "    audio_input = {\n",
    "        \"features\": m.to(0),\n",
    "        \"attn_masks\": a.to(0),\n",
    "    }\n",
    "    test = supcon_model([[\"you are my best friend, love you!\"],audio_input, None])\n",
    "    print(torch.dot(F.normalize(test[\"x_audio_wide\"][0, :], dim=0), F.normalize(test[\"x_audio_wide\"][1, :], dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supcon_model.load_state_dict(torch.load('./pytorch_model_AudioTextCLIPvFinal_epoch_25_only_meld.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supcon_model.audio_encoder.clusterization_model = kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J92hem554uXS"
   },
   "outputs": [],
   "source": [
    "df_train_f =df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIKswbRmlFZV"
   },
   "outputs": [],
   "source": [
    "#df_dev_audio = pd.concat([df_meld_dev, test_audio], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param: Select dataset for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_audio_repeated = pd.concat([df_train_audio, df_train_audio,df_train_audio,df_train_audio,df_train_audio,df_train_audio,df_train_audio, df_train_audio,df_train_audio,df_train_audio,df_train_audio,df_train_audio], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "#test_audio_repeated = pd.concat([df_dev_audio, df_dev_audio,df_dev_audio,df_dev_audio,df_dev_audio,df_dev_audio], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(torch.Tensor(list(range(len(train_df_erc)))))\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=1024, shuffle=False)\n",
    "\n",
    "test_ds = torch.utils.data.TensorDataset(torch.Tensor(list(range(len(test_df_erc)))))\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "supcon_model.eval()\n",
    "\n",
    "proj_val = []\n",
    "targets_val = []\n",
    "\n",
    "proj_train = []\n",
    "targets_train = []\n",
    "\n",
    "for i, batch_indices in enumerate(tqdm(train_loader, total=len(train_loader))):\n",
    "    with torch.no_grad():\n",
    "        batch = train_df_erc.iloc[batch_indices[0]]\n",
    "        only_text = batch[batch[\"path\"].isna()]\n",
    "        sentences = only_text[\"text\"].tolist()\n",
    "        y_text = torch.Tensor(lab_encoder.transform(only_text[\"label\"]))\n",
    "\n",
    "        only_audio = batch[batch[\"text\"].isna()]\n",
    "        audio_paths = only_audio[\"path\"].tolist()\n",
    "        try:\n",
    "            mfccs, att = audio_tokenizer.batch_tokenize(audio_paths)\n",
    "\n",
    "            audio_input = {\n",
    "                \"features\": mfccs.to(0),\n",
    "                \"attn_masks\": att.to(0),\n",
    "            }\n",
    "        except:\n",
    "            audio_input = None\n",
    "\n",
    "        y_audio = torch.Tensor(lab_encoder.transform(only_audio[\"label\"]))\n",
    "\n",
    "        mult = batch[batch[\"text\"].notna()]\n",
    "        mult = mult[mult[\"path\"].notna()]\n",
    "        mult = mult[mult[\"label\"].notna()]\n",
    "        y_mult = torch.Tensor(lab_encoder.transform(mult[\"label\"]))\n",
    "\n",
    "        audio_path_mult = [str(t['path']) for _, t in mult.iterrows()]\n",
    "\n",
    "        mfccs_mult, att_mult = audio_tokenizer.batch_tokenize(audio_path_mult)\n",
    "\n",
    "        sentences_mult = [str(t['text']) for _, t in mult.iterrows()]\n",
    "\n",
    "        multimodal = {'sentences': sentences_mult, \n",
    "                      'audio_input': {\"features\": mfccs_mult.to(0), \"attn_masks\": att_mult.to(0)}}\n",
    "\n",
    "        target = torch.cat([y_text, y_audio, y_mult])\n",
    "\n",
    "        x = [sentences, audio_input, multimodal]\n",
    "\n",
    "        if len(sentences) == 0:\n",
    "            x[0] = None\n",
    "        if len(audio_paths) == 0:\n",
    "            x[1] = None\n",
    "        if len(sentences_mult) == 0:\n",
    "            x[2] = None\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16) as autocast, torch.backends.cuda.sdp_kernel(enable_flash=False) as disable:\n",
    "\n",
    "            out = supcon_model(x)\n",
    "\n",
    "            # Multimodal loss\n",
    "            #x_mult_wide = F.normalize(out[\"x_mult_text_wide\"] + out[\"x_mult_audio_wide\"], dim=-1)\n",
    "            x_mult_wide = F.normalize(out[\"x_mult_audio_wide\"], dim=-1)\n",
    "\n",
    "        proj_train.append(np.array(x_mult_wide.detach().cpu()))\n",
    "        targets_train.append(np.array(target.cpu()))\n",
    "\n",
    "        del x_mult_wide\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "proj_train = np.concatenate(proj_train, axis=0)\n",
    "targets_train = np.concatenate(targets_train, axis=0)\n",
    "\n",
    "clf = FaissKNeighbors(k=128)\n",
    "clf.fit(proj_train, np.array(targets_train, dtype=int))\n",
    "\n",
    "supcon_model.eval()\n",
    "preds = []\n",
    "targets = []\n",
    "css = 0.0\n",
    "\n",
    "for i, batch_indices in enumerate(tqdm(test_loader, total=len(test_loader))):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        multimodal_batch = test_df_erc.iloc[batch_indices[0]]\n",
    "\n",
    "        audio_path_mult = [str(t['path']) for _, t in multimodal_batch.iterrows()]\n",
    "        mfccs_mult, att_mult = audio_tokenizer.batch_tokenize(audio_path_mult)\n",
    "\n",
    "        sentences_mult = [str(t['text']) for _, t in multimodal_batch.iterrows()]\n",
    "\n",
    "        multimodal = {'sentences': sentences_mult, \n",
    "                      'audio_input': {\"features\": mfccs_mult.to(0), \"attn_masks\": att_mult.to(0)}}\n",
    "\n",
    "        target = torch.Tensor(lab_encoder.transform(list(multimodal_batch[\"label\"])))\n",
    "\n",
    "        x = [None, None, multimodal]\n",
    "        with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16) as autocast, torch.backends.cuda.sdp_kernel(enable_flash=False) as disable:\n",
    "            out = supcon_model(x)\n",
    "\n",
    "            # Multimodal loss\n",
    "            #out_x_wide = F.normalize(out[\"x_mult_text_wide\"] + out[\"x_mult_audio_wide\"], dim=-1)\n",
    "            out_x_wide = F.normalize(out[\"x_mult_audio_wide\"], dim=-1)\n",
    "\n",
    "        cs = F.cosine_similarity(out[\"x_mult_text_wide\"], out[\"x_mult_audio_wide\"])\n",
    "\n",
    "        wide = np.array(out_x_wide.cpu())\n",
    "        pred = clf.predict(wide)\n",
    "        preds.append(pred)\n",
    "\n",
    "        assert len(wide) == len(pred)\n",
    "\n",
    "        proj_val.append(wide)\n",
    "        targets_val.append(np.array(target.cpu()))\n",
    "        css += np.sum(np.array(cs.cpu()))\n",
    "        del out_x_wide\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "proj_val = np.concatenate(proj_val, axis=0)\n",
    "targets_val = np.concatenate(targets_val, axis=0)\n",
    "\n",
    "preds = np.array(np.concatenate(preds, axis=0))\n",
    "\n",
    "css = css / len(test_df_erc)\n",
    "\n",
    "general_f1 = f1_score(targets_val, preds, average='weighted')\n",
    "general_acc = accuracy_score(targets_val, preds)\n",
    "\n",
    "print(f'Cosine Similarity between mods: {css}')\n",
    "\n",
    "meld_idx = test_df_erc[test_df_erc[\"source\"] == \"meld\"].index\n",
    "iemocap_idx = test_df_erc[test_df_erc[\"source\"] != \"meld\"].index\n",
    "\n",
    "general_f1_iemocap = f1_score(targets_val[iemocap_idx], preds[iemocap_idx], average='weighted')\n",
    "general_acc_iemocap = accuracy_score(targets_val[iemocap_idx], preds[iemocap_idx])\n",
    "\n",
    "general_f1_meld = f1_score(targets_val[meld_idx], preds[meld_idx], average='weighted')\n",
    "general_acc_meld = accuracy_score(targets_val[meld_idx], preds[meld_idx])\n",
    "\n",
    "print(f'General - KNN F1: {general_f1} Acc: {general_acc}')\n",
    "print(f'Iemocap - KNN F1: {general_f1_iemocap} Acc: {general_acc_iemocap}')\n",
    "print(f'Meld - KNN F1: {general_f1_meld} Acc: {general_acc_meld}')\n",
    "\n",
    "print(f\"Iemocap - KNN F1 (macro): {f1_score(targets_val[iemocap_idx], preds[iemocap_idx], average='macro')}\")\n",
    "print(f\"Meld - KNN F1 (macro): {f1_score(targets_val[meld_idx], preds[meld_idx], average='macro')}\")\n",
    "\n",
    "tsne = TSNE(n_components=2, learning_rate='auto', init='pca', perplexity=5).fit_transform(proj_val)\n",
    "\n",
    "sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=lab_encoder.inverse_transform(list(np.array(targets_val, dtype=int))) , palette='tab10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_erc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meld_train_idx = train_df_erc[train_df_erc[\"path\"].apply(lambda x: True if \"MELD\" in x else False)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iemocap_train_idx = train_df_erc[train_df_erc[\"path\"].apply(lambda x: False if \"MELD\" in x else True)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mean_test = proj_train[meld_train_idx].mean(axis=0)\n",
    "std_test = proj_train[meld_train_idx].std(axis=0)\n",
    "clf = FaissKNeighbors(k=128)\n",
    "clf.fit((proj_train[meld_train_idx]-mean_test)/std_test, np.array(targets_train[meld_train_idx], dtype=int))\n",
    "\n",
    "preds = clf.predict((proj_val-mean_test)/std_test)\n",
    "\n",
    "general_f1_meld = f1_score(targets_val[meld_idx], preds[meld_idx], average='weighted')\n",
    "general_acc_meld = accuracy_score(targets_val[meld_idx], preds[meld_idx])\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(targets_val[meld_idx], preds[meld_idx], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(targets_val[iemocap_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(lab_encoder.inverse_transform(np.array(targets_val[iemocap_idx], dtype=int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iemocap_orig = pd.read_json(\"emotions.json\").reset_index(drop=False)\n",
    "df_iemocap_orig = pd.melt(df_iemocap_orig, id_vars=['index'], value_vars=['train', 'val', 'test']).dropna().drop(columns=[\"variable\"]).rename(columns={\"index\":\"id\", \"value\": \"orig_label\"}).reset_index(drop=True)\n",
    "df_iemocap_orig = df_iemocap_orig[df_iemocap_orig[\"orig_label\"].notna() & (df_iemocap_orig[\"orig_label\"] != \"undecided\")].reset_index(drop=True)\n",
    "df_iemocap_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_shit(x):\n",
    "    if \"MELD\" in x:\n",
    "        return None\n",
    "    x = x.replace(\"val/\", \"\")\n",
    "    x = x.replace(\"train/\", \"\")\n",
    "    x = x.replace(\"test/\", \"\")\n",
    "    l = len(\"/home/vmachado/Documents/multimodal-datasets/IEMOCAP/raw-audios/\")\n",
    "    return x[l:].replace(\".wav\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df_erc_iemocap = train_df_erc[train_df_erc[\"path\"].apply(lambda x: True if \"IEMOCAP\" in x else False)]\n",
    "train_df_erc[\"id\"] = train_df_erc[\"path\"].apply(cleaning_shit)\n",
    "train_df_erc_iemocap = train_df_erc.dropna()\n",
    "train_df_erc_iemocap = train_df_erc_iemocap.merge(df_iemocap_orig, on=\"id\", how=\"inner\").dropna()\n",
    "train_df_erc_iemocap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iemocap_train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_erc[\"id\"] = test_df_erc[\"path\"].apply(lambda x: x[len('/home/vmachado/Documents/multimodal-datasets/IEMOCAP/raw-audios/test/'):].replace(\".wav\", \"\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_erc_iemocap = test_df_erc[test_df_erc[\"source\"] == \"iemocap\"].reset_index(drop=True)\n",
    "test_df_erc_iemocap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_erc_iemocap = test_df_erc_iemocap.merge(df_iemocap_orig, on=\"id\", how=\"inner\")\n",
    "test_df_erc_iemocap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lab = LabelEncoder().fit(train_df_erc_iemocap[\"orig_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_erc_iemocap[\"orig_label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_erc_iemocap[\"orig_label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_erc_iemocap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_erc_iemocap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels_train = new_lab.transform(train_df_erc_iemocap[\"orig_label\"])\n",
    "correct_labels_test = new_lab.transform(test_df_erc_iemocap[\"orig_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_erc[test_df_erc[\"source\"] == \"iemocap\"][\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mean_test = proj_train[iemocap_train_idx].mean(axis=0)\n",
    "std_test = proj_train[iemocap_train_idx].std(axis=0)\n",
    "clf = FaissKNeighbors(k=128)\n",
    "clf.fit((proj_train[iemocap_train_idx]-mean_test)/std_test, np.array(targets_train[iemocap_train_idx], dtype=int))\n",
    "\n",
    "preds = clf.predict((proj_val-mean_test)/std_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(targets_val[iemocap_idx], preds[iemocap_idx], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_f1_iemocap = f1_score(targets_val[iemocap_idx], preds[iemocap_idx], average='weighted')\n",
    "general_acc_iemocap = accuracy_score(targets_val[iemocap_idx], preds[iemocap_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_f1_iemocap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_f1_meld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(targets_val[iemocap_idx], list(map(lambda x: x if x != 1 else 6, preds[iemocap_idx])), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b1501f7bf4a4b509267687b71bca3bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4dbc238f1304ecba571deabd74ff630",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_154effefd76744f1847a4e90cd50adbe",
      "value": 112
     }
    },
    "0b478c1f483e4fccbb6f56f002234dea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e92d24aa67a4463843f09da7d592c94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f4366c0c2f74505b5268165f3ff15b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "104351e0f60c4dceacc51617d69cdef2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d800f0793034ec29c37ac6e35ab3375",
      "placeholder": "​",
      "style": "IPY_MODEL_5c5460fe4d344d9aaa3e87e41ca08164",
      "value": " 629/629 [00:00&lt;00:00, 47.2kB/s]"
     }
    },
    "147da8015a5744b293df28397d0cb08c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "154effefd76744f1847a4e90cd50adbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "178de3d498054417b5f3661675d2deff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ec467cae94e4f8fa22bc3d2bc563e85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "381e15620779478db7fcf1bd2d124c1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bf3f371933f4d328efa23edc61f9f46",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ffeda0d9845d4610aa25c47c14ad90f1",
      "value": 231508
     }
    },
    "39e75864886e4a6c8ca47a53375d1e55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_579642ddd5bf47e2861f1c7ae034f2a0",
      "max": 69583549,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a8f4bfce2b534d7b9a302ecb6986609d",
      "value": 69583549
     }
    },
    "3f40ded8f3884d468803b5cbb92d04af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b47e5878d64c40519b6cb51321b90f62",
      "max": 314,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ef30cb62a33341cfa92780794e578fa8",
      "value": 314
     }
    },
    "3fad1ce87a9f4b1baad4c056bd76d0ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98b25709d1f34983bf771a1bbd3dae3d",
      "placeholder": "​",
      "style": "IPY_MODEL_bcfbcdafd7dd4d77a26037458e909fe5",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "5341a0d3bcaa4f44a3bc1673411b8135": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3fad1ce87a9f4b1baad4c056bd76d0ad",
       "IPY_MODEL_3f40ded8f3884d468803b5cbb92d04af",
       "IPY_MODEL_b272914175c14c5494bbd466ced2c24c"
      ],
      "layout": "IPY_MODEL_7952be014e164b07ab81898a97fbc331"
     }
    },
    "563e4f1cc475402080aa71949a012f49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "579642ddd5bf47e2861f1c7ae034f2a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c5460fe4d344d9aaa3e87e41ca08164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "671767bc077146a1aeed5607c971d138": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b8ec460c2694c0ab4ed319f3d11e119": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6eba96636d54bbbbb87665595f9d96d",
      "max": 466248,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9532f4e341a4503abe2affac1f8e88e",
      "value": 466248
     }
    },
    "6d00b5d46c58434cb2dedd09f975e504": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70b8347a3a2c45b2b78a0117ec52e7ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "74dfc96b83f34e3c8403c192f423a6e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7506d573de6c4844bbf5f4af7947424b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "765d7438bce044758c9b1b818ede9ac4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77d8310b1be34cf594a4731b9845e2a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_862bfc472db84bfc8f031fd3a76e9809",
       "IPY_MODEL_381e15620779478db7fcf1bd2d124c1b",
       "IPY_MODEL_ea5c189c9660499f844a6fe3c90c59bf"
      ],
      "layout": "IPY_MODEL_0b478c1f483e4fccbb6f56f002234dea"
     }
    },
    "7952be014e164b07ab81898a97fbc331": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bd6fbe38ada40d5a18dcfa021326ea5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9fbb36b544104cf2be8d7a6aa7cf28e1",
       "IPY_MODEL_6b8ec460c2694c0ab4ed319f3d11e119",
       "IPY_MODEL_93068874e3c3415bb750db3439d9cf6a"
      ],
      "layout": "IPY_MODEL_bd3ced1d62224f7185d14373ad07cdff"
     }
    },
    "85f4be5ebdc94c7e8001a083a7c40154": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "862bfc472db84bfc8f031fd3a76e9809": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0fe5abc20594822bba649da177c106b",
      "placeholder": "​",
      "style": "IPY_MODEL_70b8347a3a2c45b2b78a0117ec52e7ae",
      "value": "Downloading (…)solve/main/vocab.txt: 100%"
     }
    },
    "87ef10ff662a4ea0994296b0c8577c3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ca460fa5f584d9c88875eeb30eac76a",
      "max": 629,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b288324ff1fb4eeeb359d44cb1a40d12",
      "value": 629
     }
    },
    "89cfc81fe5464ee9abb0a9a1a1b9d141": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d800f0793034ec29c37ac6e35ab3375": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91c7531f525b4662844c113817a5bd10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_563e4f1cc475402080aa71949a012f49",
      "placeholder": "​",
      "style": "IPY_MODEL_ba6d485b3d364808894dc2336d9fd147",
      "value": "Downloading (…)cial_tokens_map.json: 100%"
     }
    },
    "93068874e3c3415bb750db3439d9cf6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_147da8015a5744b293df28397d0cb08c",
      "placeholder": "​",
      "style": "IPY_MODEL_fa17642f8dcd425e87a7ea27c7782afd",
      "value": " 466k/466k [00:00&lt;00:00, 526kB/s]"
     }
    },
    "98b25709d1f34983bf771a1bbd3dae3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9adf9f212e5945b58ff185047565bcf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bf3f371933f4d328efa23edc61f9f46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ca460fa5f584d9c88875eeb30eac76a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fbb36b544104cf2be8d7a6aa7cf28e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74dfc96b83f34e3c8403c192f423a6e3",
      "placeholder": "​",
      "style": "IPY_MODEL_85f4be5ebdc94c7e8001a083a7c40154",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    },
    "a41e0d7fd35c44bd80808903ea66cb61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91c7531f525b4662844c113817a5bd10",
       "IPY_MODEL_0b1501f7bf4a4b509267687b71bca3bb",
       "IPY_MODEL_d860f362d07041228fb279ba07a48a75"
      ],
      "layout": "IPY_MODEL_d135bd21ef514eeb95d91f8629bf7688"
     }
    },
    "a8f4bfce2b534d7b9a302ecb6986609d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b272914175c14c5494bbd466ced2c24c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7506d573de6c4844bbf5f4af7947424b",
      "placeholder": "​",
      "style": "IPY_MODEL_178de3d498054417b5f3661675d2deff",
      "value": " 314/314 [00:00&lt;00:00, 17.4kB/s]"
     }
    },
    "b288324ff1fb4eeeb359d44cb1a40d12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3df00cb8ab94fcf8b435765b8a9133b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d00b5d46c58434cb2dedd09f975e504",
      "placeholder": "​",
      "style": "IPY_MODEL_0e92d24aa67a4463843f09da7d592c94",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "b47e5878d64c40519b6cb51321b90f62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6b140015ed746968a3f7bd4210ee5c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6eba96636d54bbbbb87665595f9d96d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9b51ab0d78545739a420b73e0231b1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba6d485b3d364808894dc2336d9fd147": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc1c88900f37416fa8ebec58ec0e8ff3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbdc5f2631d94d399c00033043711360",
      "placeholder": "​",
      "style": "IPY_MODEL_b6b140015ed746968a3f7bd4210ee5c5",
      "value": " 69.6M/69.6M [00:00&lt;00:00, 165MB/s]"
     }
    },
    "bcfbcdafd7dd4d77a26037458e909fe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd3ced1d62224f7185d14373ad07cdff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4dbc238f1304ecba571deabd74ff630": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9532f4e341a4503abe2affac1f8e88e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cbdc5f2631d94d399c00033043711360": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf773a5a5a7c43d58957c19654160f6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0fe5abc20594822bba649da177c106b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d135bd21ef514eeb95d91f8629bf7688": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6395a6be7334dc4a656a62fea25df3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b3df00cb8ab94fcf8b435765b8a9133b",
       "IPY_MODEL_39e75864886e4a6c8ca47a53375d1e55",
       "IPY_MODEL_bc1c88900f37416fa8ebec58ec0e8ff3"
      ],
      "layout": "IPY_MODEL_9adf9f212e5945b58ff185047565bcf3"
     }
    },
    "d860f362d07041228fb279ba07a48a75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f4366c0c2f74505b5268165f3ff15b5",
      "placeholder": "​",
      "style": "IPY_MODEL_2ec467cae94e4f8fa22bc3d2bc563e85",
      "value": " 112/112 [00:00&lt;00:00, 7.45kB/s]"
     }
    },
    "ea5c189c9660499f844a6fe3c90c59bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_671767bc077146a1aeed5607c971d138",
      "placeholder": "​",
      "style": "IPY_MODEL_cf773a5a5a7c43d58957c19654160f6f",
      "value": " 232k/232k [00:00&lt;00:00, 341kB/s]"
     }
    },
    "ef30cb62a33341cfa92780794e578fa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ef6d4a9a190c411198829adc42a4e3c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ef74f18be5a940ba8696edb399e3b6f7",
       "IPY_MODEL_87ef10ff662a4ea0994296b0c8577c3d",
       "IPY_MODEL_104351e0f60c4dceacc51617d69cdef2"
      ],
      "layout": "IPY_MODEL_b9b51ab0d78545739a420b73e0231b1d"
     }
    },
    "ef74f18be5a940ba8696edb399e3b6f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_765d7438bce044758c9b1b818ede9ac4",
      "placeholder": "​",
      "style": "IPY_MODEL_89cfc81fe5464ee9abb0a9a1a1b9d141",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "fa17642f8dcd425e87a7ea27c7782afd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ffeda0d9845d4610aa25c47c14ad90f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
